{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058114fe",
   "metadata": {},
   "source": [
    "## 0.For_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af001030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and file written to /nfs/research/goldman/zihao/code/.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/nfs/research/goldman/zihao/compViridian_2/MAPLE_Part/For_col/MAPLE0.3.2_rateVar_checkingErrors_col_estimatedErrors.txt\"\n",
    "output_folder = \"/nfs/research/goldman/zihao/code/\"\n",
    "\n",
    "# Open the output file to write the processed data\n",
    "with open(os.path.join(output_folder, \"output_modified.txt\"), \"w\") as output_file:\n",
    "    # Write the column headers to the output file\n",
    "    output_file.write(\"ID\\tPosition\\n\")\n",
    "    \n",
    "    # Initialize the current_id variable to None\n",
    "    current_id = None\n",
    "    \n",
    "    # Iterate over each line in the file and convert it to a list\n",
    "    with open(file_path, \"r\") as input_file:\n",
    "        for line in input_file:\n",
    "            if line.startswith(\">\"):\n",
    "                # Update current_id if a new ID is encountered\n",
    "                current_id = line[1:].strip()\n",
    "            else:\n",
    "                tokens = line.strip().split()\n",
    "                position = int(tokens[0])\n",
    "                base = tokens[1]\n",
    "                percentage = float(tokens[2])\n",
    "\n",
    "                # Check if percentage is less than 0.5, and if so, skip adding it to the processed data\n",
    "                if percentage < 0.5:\n",
    "                    continue\n",
    "\n",
    "                # Write the processed data to the output file\n",
    "                output_file.write(f\"{current_id}\\t{position}\\n\")\n",
    "                \n",
    "                # Delete variables to free up memory\n",
    "                del tokens, base, position, percentage\n",
    "                \n",
    "    # Delete the current_id variable after the loop has finished\n",
    "    del current_id\n",
    "\n",
    "print(f\"Processing complete and file written to {output_folder}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "158e631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of MAPLE-marked error positions is: 44459\n"
     ]
    }
   ],
   "source": [
    "file_path = '/nfs/research/goldman/zihao/code/output_modified.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(f\"The number of MAPLE-marked error positions is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ccc3a",
   "metadata": {},
   "source": [
    "## 1.For_COV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f870b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def search_position_value_and_get_column_4(file_path, id_position):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        file_header = f.readline().strip().split(\"\\t\")\n",
    "        position_column_index = file_header.index(\"Position\")\n",
    "        column_4_index = 2\n",
    "\n",
    "        for line in f:\n",
    "            line_values = line.strip().split(\"\\t\")\n",
    "            if position_column_index < len(line_values): # Check if index is within range\n",
    "                position_value = int(line_values[position_column_index])\n",
    "                if position_value == id_position and column_4_index < len(line_values):\n",
    "                    return line_values[column_4_index]\n",
    "    return None\n",
    "\n",
    "def calculate_mean_error(data):\n",
    "    position_error_sum = {}\n",
    "    position_count = {}\n",
    "\n",
    "    for line in data:\n",
    "        position = line[1]\n",
    "        mean_error = line[-1]\n",
    "\n",
    "        if mean_error != \"None\":\n",
    "            mean_error = float(mean_error)\n",
    "\n",
    "            if position in position_error_sum:\n",
    "                position_error_sum[position] += mean_error\n",
    "                position_count[position] += 1\n",
    "            else:\n",
    "                position_error_sum[position] = mean_error\n",
    "                position_count[position] = 1\n",
    "\n",
    "    mean_error_by_position = {}\n",
    "    for position in position_error_sum:\n",
    "        mean_error_by_position[position] = position_error_sum[position] / position_count[position]\n",
    "\n",
    "    return mean_error_by_position\n",
    "\n",
    "def process_data(data_file_path, directory_to_search, output_file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(data_file_path, \"r\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "        header.append(\"MEAN_err\")\n",
    "\n",
    "        for line in f:\n",
    "            data.append(line.strip().split(\"\\t\") + [None])\n",
    "\n",
    "    for line in data:\n",
    "        id_now = str(line[0])\n",
    "        id_position = line[1]\n",
    "\n",
    "        if id_position is not None:\n",
    "            for filename in os.listdir(directory_to_search):\n",
    "                if id_now in filename and filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(directory_to_search, filename)\n",
    "                    result = search_position_value_and_get_column_4(file_path, int(id_position))\n",
    "                    if result is not None:\n",
    "                        line[-1] = result\n",
    "                        break\n",
    "\n",
    "    # Remove rows with \"None\" or None in the MEAN_err column\n",
    "    data = [line for line in data if line[-1] is not None and line[-1] != \"None\"]\n",
    "\n",
    "    # Calculate the mean error for each position\n",
    "    mean_error_by_position = calculate_mean_error(data)\n",
    "\n",
    "    # Write the result to the output file\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        f.write(\"Position\\tMEAN_err\\n\")\n",
    "        for position, mean_error in mean_error_by_position.items():\n",
    "            f.write(f\"{position}\\t{mean_error}\\n\")\n",
    "            \n",
    "directory_to_search = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "data_file_path = os.path.join(output_folder, \"output_modified.txt\")\n",
    "output_file_path = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing_forCoverage/COV_dataProcessing_errorPos.txt\"\n",
    "\n",
    "process_data(data_file_path, directory_to_search, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f625b55",
   "metadata": {},
   "source": [
    "## 2.For_ANNOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2e9c7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_folder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(row) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     data1_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutput_folder\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_modified.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     data_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing_forCoverage/ANNOT_dataProcessing_errorPos.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_folder' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data1_file, data_folder, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def read_data1(self):\n",
    "        \"\"\"Read data1 from file and return the header and data.\"\"\"\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Process data1 by looking up corresponding rows in data folder.\"\"\"\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        \"\"\"Write the processed data1 to the output file.\"\"\"\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    data1_file = os.path.join(output_folder, \"output_modified.txt\")\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing_forCoverage/ANNOT_dataProcessing_errorPos.txt\"\n",
    "\n",
    "    processor = DataProcessor(data1_file, data_folder, output_file)\n",
    "    processor.write_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345edfe2",
   "metadata": {},
   "source": [
    "### [!!!]Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45a59f",
   "metadata": {},
   "source": [
    "###### Run the program\n",
    "\n",
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/errorChecking_errPos_dataProcessing.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/outputChecking_errPos_dataProcessing.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0839ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class MapleFileProcessor:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.output_file, \"w\") as output_file:\n",
    "            output_file.write(\"ID\\tPosition\\n\")\n",
    "            current_id = None\n",
    "            with open(self.input_file, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    if line.startswith(\">\"):\n",
    "                        current_id = line[1:].strip()\n",
    "                    else:\n",
    "                        tokens = line.strip().split()\n",
    "                        position = int(tokens[0])\n",
    "                        percentage = float(tokens[2])\n",
    "                        if percentage < 0.5:\n",
    "                            continue\n",
    "                        output_file.write(f\"{current_id}\\t{position}\\n\")\n",
    "\n",
    "\n",
    "class CovDataProcessor:\n",
    "    def __init__(self, data_file_path, dir_to_search_Cov, output_file_path):\n",
    "        self.data_file_path = data_file_path\n",
    "        self.dir_to_search_Cov = dir_to_search_Cov\n",
    "        self.output_file_path = output_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _search_position_value_and_get_column_4(file_path, id_position):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            file_header = f.readline().strip().split(\"\\t\")\n",
    "            position_column_index = file_header.index(\"Position\")\n",
    "            column_4_index = 2\n",
    "            for line in f:\n",
    "                line_values = line.strip().split(\"\\t\")\n",
    "                if position_column_index < len(line_values):\n",
    "                    position_value = int(line_values[position_column_index])\n",
    "                    if position_value == id_position and column_4_index < len(line_values):\n",
    "                        return line_values[column_4_index]\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_mean_error(data):\n",
    "        position_error_sum = {}\n",
    "        position_count = {}\n",
    "        for line in data:\n",
    "            position = line[1]\n",
    "            mean_error = line[-1]\n",
    "            if mean_error != \"None\":\n",
    "                mean_error = float(mean_error)\n",
    "                if position in position_error_sum:\n",
    "                    position_error_sum[position] += mean_error\n",
    "                    position_count[position] += 1\n",
    "                else:\n",
    "                    position_error_sum[position] = mean_error\n",
    "                    position_count[position] = 1\n",
    "        mean_error_by_position = {}\n",
    "        for position in position_error_sum:\n",
    "            mean_error_by_position[position] = position_error_sum[position] / position_count[position]\n",
    "        return mean_error_by_position\n",
    "\n",
    "    def process(self):\n",
    "        data = []\n",
    "        with open(self.data_file_path, \"r\") as f:\n",
    "            header = f.readline().strip().split(\"\\t\")\n",
    "            header.append(\"COV_Ratio\")\n",
    "            for line in f:\n",
    "                data.append(line.strip().split(\"\\t\") + [None])\n",
    "        for line in data:\n",
    "            id_now = str(line[0])\n",
    "            id_position = line[1]\n",
    "            if id_position is not None:\n",
    "                for filename in os.listdir(self.dir_to_search_Cov):\n",
    "                    if id_now in filename and filename.endswith(\".txt\"):\n",
    "                        file_path = os.path.join(self.dir_to_search_Cov, filename)\n",
    "                        result = self._search_position_value_and_get_column_4(file_path, int(id_position))\n",
    "                        if result is not None:\n",
    "                            line[-1] = result\n",
    "                            break\n",
    "        data = [line for line in data if line[-1] is not None and line[-1] != \"None\"]\n",
    "        mean_error_by_position = self._calculate_mean_error(data)\n",
    "        with open(self.output_file_path, \"w\") as f:\n",
    "            f.write(\"Position\\tCOV_Ratio\\n\")\n",
    "            for position, mean_error in mean_error_by_position.items():\n",
    "                f.write(f\"{position}\\t{mean_error}\\n\")\n",
    "\n",
    "\n",
    "class AnnotDataProcessor:\n",
    "    def __init__(self, data1_file, dir_to_search_Annot, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.dir_to_search_Annot = dir_to_search_Annot\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def _read_data1(self):\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "\n",
    "    def _process_data(self):\n",
    "        header, data1 = self._read_data1()\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.dir_to_search_Annot, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "        return header, data1\n",
    "\n",
    "    def process(self):\n",
    "        header, data1 = self._process_data()\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = \"/nfs/research/goldman/zihao/compViridian_2/MAPLE_Part/For_col/MAPLE0.3.2_rateVar_checkingErrors_col_estimatedErrors.txt\"\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/\"\n",
    "    output_file = os.path.join(output_folder, \"output_modified.txt\")\n",
    "    mapleProcessor = MapleFileProcessor(input_file, output_file)\n",
    "    mapleProcessor.process()\n",
    "\n",
    "    dir_to_search_Cov = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    data_file_path = output_file\n",
    "    output_file_cov = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/COV_dataProcessing_errorPos.txt\"\n",
    "    covProcessor = CovDataProcessor(data_file_path, dir_to_search_Cov, output_file_cov)\n",
    "    covProcessor.process()\n",
    "\n",
    "    data1_file = output_file\n",
    "    dir_to_search_Annot = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file_annot = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_errorPos.txt\"\n",
    "    annotProcessor = AnnotDataProcessor(data1_file, dir_to_search_Annot, output_file_annot)\n",
    "    annotProcessor.process()\n",
    "    \n",
    "\n",
    "with open(output_file, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(f\"The number of MAPLE-marked error positions is: {line_count-1}\")\n",
    "\n",
    "with open(output_file_cov, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "print(f\"The number of MAPLE-marked error positions \"\n",
    "      f\"(After finishing the COV information merge operation) is: {line_count-1}\")\n",
    "\n",
    "with open(output_file_annot, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "print(f\"The number of MAPLE-marked error positions \"\n",
    "      f\"(After finishing the ANNOT information merge operation) is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe0556",
   "metadata": {},
   "source": [
    "###### For output\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forOutput.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forOutput.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forOutput.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d6860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class MapleFileProcessor:\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def process(self):\n",
    "        with open(self.output_file, \"w\") as output_file:\n",
    "            output_file.write(\"ID\\tPosition\\n\")\n",
    "            current_id = None\n",
    "            with open(self.input_file, \"r\") as input_file:\n",
    "                for line in input_file:\n",
    "                    if line.startswith(\">\"):\n",
    "                        current_id = line[1:].strip()\n",
    "                    else:\n",
    "                        tokens = line.strip().split()\n",
    "                        position = int(tokens[0])\n",
    "                        percentage = float(tokens[2])\n",
    "                        if percentage < 0.5:\n",
    "                            continue\n",
    "                        output_file.write(f\"{current_id}\\t{position}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = \"/nfs/research/goldman/zihao/compViridian_2/MAPLE_Part/For_col/MAPLE0.3.2_rateVar_checkingErrors_col_estimatedErrors.txt\"\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/\"\n",
    "    output_file = os.path.join(output_folder, \"output_modified.txt\")\n",
    "    mapleProcessor = MapleFileProcessor(input_file, output_file)\n",
    "    mapleProcessor.process()\n",
    "\n",
    "with open(output_file, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "print(f\"The number of MAPLE-marked error positions is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50ac84",
   "metadata": {},
   "source": [
    "###### For cov\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forCov.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forCov.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forCov.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CovDataProcessor:\n",
    "    def __init__(self, data_file_path, dir_to_search_Cov, output_file_path):\n",
    "        self.data_file_path = data_file_path\n",
    "        self.dir_to_search_Cov = dir_to_search_Cov\n",
    "        self.output_file_path = output_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _search_position_value_and_get_column_4(file_path, id_position):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            file_header = f.readline().strip().split(\"\\t\")\n",
    "            position_column_index = file_header.index(\"Position\")\n",
    "            column_4_index = 2\n",
    "            for line in f:\n",
    "                line_values = line.strip().split(\"\\t\")\n",
    "                if position_column_index < len(line_values):\n",
    "                    position_value = int(line_values[position_column_index])\n",
    "                    if position_value == id_position and column_4_index < len(line_values):\n",
    "                        return line_values[column_4_index]\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_mean_error(data):\n",
    "        position_error_sum = {}\n",
    "        position_count = {}\n",
    "        for line in data:\n",
    "            position = line[1]\n",
    "            mean_error = line[-1]\n",
    "            if mean_error != \"None\":\n",
    "                mean_error = float(mean_error)\n",
    "                if position in position_error_sum:\n",
    "                    position_error_sum[position] += mean_error\n",
    "                    position_count[position] += 1\n",
    "                else:\n",
    "                    position_error_sum[position] = mean_error\n",
    "                    position_count[position] = 1\n",
    "        mean_error_by_position = {}\n",
    "        for position in position_error_sum:\n",
    "            mean_error_by_position[position] = position_error_sum[position] / position_count[position]\n",
    "        return mean_error_by_position\n",
    "\n",
    "    def process(self):\n",
    "        data = []\n",
    "        with open(self.data_file_path, \"r\") as f:\n",
    "            header = f.readline().strip().split(\"\\t\")\n",
    "            header.append(\"COV_Ratio\")\n",
    "            for line in f:\n",
    "                data.append(line.strip().split(\"\\t\") + [None])\n",
    "        for line in data:\n",
    "            id_now = str(line[0])\n",
    "            id_position = line[1]\n",
    "            if id_position is not None:\n",
    "                for filename in os.listdir(self.dir_to_search_Cov):\n",
    "                    if id_now in filename and filename.endswith(\".txt\"):\n",
    "                        file_path = os.path.join(self.dir_to_search_Cov, filename)\n",
    "                        result = self._search_position_value_and_get_column_4(file_path, int(id_position))\n",
    "                        if result is not None:\n",
    "                            line[-1] = result\n",
    "                            break\n",
    "        data = [line for line in data if line[-1] is not None and line[-1] != \"None\"]\n",
    "        mean_error_by_position = self._calculate_mean_error(data)\n",
    "        with open(self.output_file_path, \"w\") as f:\n",
    "            f.write(\"Position\\tCOV_Ratio\\n\")\n",
    "            for position, mean_error in mean_error_by_position.items():\n",
    "                f.write(f\"{position}\\t{mean_error}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/\"\n",
    "    output_file = os.path.join(output_folder, \"output_modified.txt\")\n",
    "\n",
    "    \n",
    "    # Wait until the output_file exists\n",
    "    while not os.path.exists(output_file):\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "    print(\"Output file found. Proceeding with the process...\")\n",
    "    \n",
    "    dir_to_search_Cov = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    data_file_path = output_file\n",
    "    output_file_cov = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/COV_dataProcessing_errorPos.txt\"\n",
    "    covProcessor = CovDataProcessor(data_file_path, dir_to_search_Cov, output_file_cov)\n",
    "    covProcessor.process()\n",
    "\n",
    "with open(output_file_cov, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "print('=========================== information =========================== ')\n",
    "print(f\"The number of MAPLE-marked error positions \"\n",
    "      f\"(After finishing the COV information merge operation) is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58720a28",
   "metadata": {},
   "source": [
    "###### For annot\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forAnnot.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forAnnot.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forAnnot.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class AnnotDataProcessor:\n",
    "    def __init__(self, data1_file, dir_to_search_Annot, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.dir_to_search_Annot = dir_to_search_Annot\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def _read_data1(self):\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "\n",
    "    def _process_data(self):\n",
    "        header, data1 = self._read_data1()\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.dir_to_search_Annot, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "        return header, data1\n",
    "\n",
    "    def process(self):\n",
    "        header, data1 = self._process_data()\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/\"\n",
    "    output_file = os.path.join(output_folder, \"output_modified.txt\")\n",
    "\n",
    "    # Wait until the output_file exists\n",
    "    while not os.path.exists(output_file):\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "    print(\"Output file found. Proceeding with the process...\")\n",
    "    \n",
    "    data1_file = output_file\n",
    "    dir_to_search_Annot = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file_annot = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_errorPos.txt\"\n",
    "    annotProcessor = AnnotDataProcessor(data1_file, dir_to_search_Annot, output_file_annot)\n",
    "    annotProcessor.process()\n",
    "    \n",
    "with open(output_file_annot, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "print('=========================== information =========================== ')\n",
    "print(f\"The number of MAPLE-marked error positions \"\n",
    "      f\"(After finishing the ANNOT information merge operation) is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9256a9f",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/BASH_errPos_dataProcessing.sh\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
