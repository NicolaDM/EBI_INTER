{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ade10fa",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Combine to three classes\n",
    "2. Excuate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6c1e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136344"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "data_set = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            data_set.add(line.strip())\n",
    "\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1a61",
   "metadata": {},
   "source": [
    "## 1.forCoverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67650404",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, output_folder, max_copy=500):\n",
    "    \"\"\"\n",
    "    Check the files in the given folder whose filenames contain the IDs in the specified files to the output folder.\n",
    "    \"\"\"\n",
    "    id_set = set()\n",
    "\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    count = 0  # Initialize the counter\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            shutil.copy(os.path.join(folder_path, filename), os.path.join(output_folder, filename))\n",
    "            count += 1  # Increment the counter\n",
    "\n",
    "        if count >= max_copy:  # If the number of copied files reach 1000, break the loop\n",
    "            break\n",
    "            \n",
    "folder_path = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "output_folder = \"/nfs/research/goldman/zihao/code/TEST/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "check_files_with_id(folder_path, checkid_file, output_folder)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec582425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### extract\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, output_folder):\n",
    "    \"\"\"\n",
    "    Check the files in the given folder whose filenames contain the IDs in the specified files to the output folder.\n",
    "    \"\"\"\n",
    "    id_set = set()\n",
    "\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            shutil.copy(os.path.join(folder_path, filename), os.path.join(output_folder, filename))\n",
    "            \n",
    "folder_path = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/Folder_MAPLE_allPos\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "check_files_with_id(folder_path, checkid_file, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae442e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f8c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in '/nfs/research/goldman/zihao/code/TEST': 500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files(folder_path):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(\"Invalid folder path!\")\n",
    "\n",
    "    file_count = 0\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "# Provide the folder path here\n",
    "folder_path = \"/nfs/research/goldman/zihao/code/TEST\"\n",
    "\n",
    "try:\n",
    "    num_files = count_files(folder_path)\n",
    "    print(f\"Number of files in '{folder_path}': {num_files}\")\n",
    "except ValueError as e:\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13165a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_cov, 'r') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "print('=========================== information =========================== ')\n",
    "print(f\"The number of MAPLE-marked error positions \"\n",
    "      f\"(After finishing the COV information merge operation) is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0db351",
   "metadata": {},
   "source": [
    "## 2.sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b76c4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, output_file_path, k):\n",
    "        self.output_file_path = output_file_path\n",
    "        self.k = k\n",
    "\n",
    "    def generator(self, file_list):\n",
    "        for file_name in file_list:\n",
    "            with open(file_name, 'r') as file:\n",
    "                file_id = os.path.splitext(os.path.basename(file_name))[0]  # Get file name without extension\n",
    "                file_id = file_id.replace('_coverage', '')  # Remove '_coverage' from the file name\n",
    "                next(file)  # skip the header line\n",
    "                for line in file:\n",
    "                    yield [file_id] + line.strip().split('\\t')  # prepend file name to the line\n",
    "\n",
    "    def reservoir_sampling(self, stream):\n",
    "        # Initialize the reservoir for each position\n",
    "        reservoir = {}\n",
    "        for item in stream:\n",
    "            position = int(item[1])\n",
    "            if position not in reservoir:\n",
    "                reservoir[position] = [item]\n",
    "            elif len(reservoir[position]) < self.k:\n",
    "                reservoir[position].append(item)\n",
    "            else:\n",
    "                # Replace elements with gradually decreasing probability\n",
    "                j = random.randint(0, len(reservoir[position]) + 1)\n",
    "                if j < self.k:\n",
    "                    reservoir[position][j] = item\n",
    "        return reservoir\n",
    "\n",
    "    def output_samples(self, samples):\n",
    "        # Output the samples to a file\n",
    "        with open(self.output_file_path, 'w') as output_file:\n",
    "            # Write header line\n",
    "            output_file.write('ID\\tPosition\\tN\\tCov_RATIO\\n')\n",
    "\n",
    "            for position, sample_list in samples.items():\n",
    "                for sample in sample_list:\n",
    "                    output_file.write('\\t'.join(sample) + '\\n')\n",
    "\n",
    "    def process_files(self, folder_path):\n",
    "        file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "        stream = self.generator(file_list)\n",
    "        samples = self.reservoir_sampling(stream)\n",
    "        self.output_samples(samples)\n",
    "\n",
    "sampler_output = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/COV_dataProcessing_allPos.txt\"\n",
    "output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/Folder_MAPLE_allPos\"\n",
    "sampler = ReservoirSampler(sampler_output, 5)\n",
    "sampler.process_files(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0ec84",
   "metadata": {},
   "source": [
    "## 3.forAnnot【ing】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7763a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data1_file, data_folder, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "    \n",
    "    def read_data1(self):\n",
    "        \"\"\"Read data1 from file and return the header and data.\"\"\"\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "    \n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Process data1 by looking up corresponding rows in data folder.\"\"\"\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        \"\"\"Write the processed data1 to the output file.\"\"\"\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    data1_file = sampler_output\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_allPos.txt\"\n",
    "\n",
    "    processor = DataProcessor(data1_file, data_folder, output_file)\n",
    "    processor.write_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81089ea1",
   "metadata": {},
   "source": [
    "```python\n",
    "def read_data1(self):\n",
    "    \"\"\"Read data1 from file and return the header and data.\"\"\"\n",
    "    data1 = []\n",
    "    with open(self.data1_file) as f:\n",
    "        header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "        count = 0  # 计数器\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            data1.append(line)\n",
    "            count += 1\n",
    "            if count == 10:  # 达到10行后终止循环\n",
    "                break\n",
    "    return header, data1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb78d3",
   "metadata": {},
   "source": [
    "#### Sampler [Unencapsulate]\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "\n",
    "def generator(file_list):\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as file:\n",
    "            next(file)  # skip the header line\n",
    "            for line in file:\n",
    "                yield line.strip().split('\\t')  # split by tab\n",
    "\n",
    "def reservoir_sampling(stream, k):\n",
    "    # Initialize the reservoir for each position\n",
    "    reservoir = {}\n",
    "    for item in stream:\n",
    "        position = int(item[0])  # Position is in the first column\n",
    "        if position not in reservoir:\n",
    "            reservoir[position] = [item]\n",
    "        elif len(reservoir[position]) < k:\n",
    "            reservoir[position].append(item)\n",
    "        else:\n",
    "            # Replace elements with gradually decreasing probability\n",
    "            j = random.randint(0, len(reservoir[position]) + 1)\n",
    "            if j < k:\n",
    "                reservoir[position][j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Get a list of all the files in the directory\n",
    "folder_path = output_folder\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "\n",
    "# Create the generator\n",
    "stream = generator(file_list)\n",
    "\n",
    "# Reservoir sampling\n",
    "k = 5  # The number of items to sample for each position\n",
    "samples = reservoir_sampling(stream, k)\n",
    "\n",
    "# Output the samples to a file\n",
    "with open('/nfs/research/goldman/zihao/code/output.txt', 'w') as output_file:\n",
    "    for position, sample_list in samples.items():\n",
    "        for sample in sample_list:\n",
    "            output_file.write('\\t'.join(sample) + '\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe38537",
   "metadata": {},
   "source": [
    "### [!!!] Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d59a57",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/Folder_Checking/errorChecking_allPos_dataProcessing.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/Folder_Checking/outputChecking_allPos_dataProcessing.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/CODE_allPos_dataProcessing.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "class CovFileProcessor:\n",
    "    def __init__(self, folder_path, checkid_file, output_folder):\n",
    "        self.folder_path = folder_path\n",
    "        self.checkid_file = checkid_file\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "    def run(self):\n",
    "        id_set = set()\n",
    "\n",
    "        with open(self.checkid_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    id_set.add(line[1:])\n",
    "\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "        for filename in os.listdir(self.folder_path):\n",
    "            if any(id_str in filename for id_str in id_set):\n",
    "                shutil.copy(os.path.join(self.folder_path, filename), os.path.join(self.output_folder, filename))\n",
    "\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, output_file_path, k):\n",
    "        self.output_file_path = output_file_path\n",
    "        self.k = k\n",
    "\n",
    "    def generator(self, file_list):\n",
    "        for file_name in file_list:\n",
    "            with open(file_name, 'r') as file:\n",
    "                file_id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "                file_id = file_id.replace('_coverage', '')\n",
    "                next(file)\n",
    "                for line in file:\n",
    "                    yield [file_id] + line.strip().split('\\t')\n",
    "\n",
    "    def reservoir_sampling(self, stream):\n",
    "        reservoir = {}\n",
    "        for item in stream:\n",
    "            position = int(item[1])\n",
    "            if position not in reservoir:\n",
    "                reservoir[position] = [item]\n",
    "            elif len(reservoir[position]) < self.k:\n",
    "                reservoir[position].append(item)\n",
    "            else:\n",
    "                j = random.randint(0, len(reservoir[position]) + 1)\n",
    "                if j < self.k:\n",
    "                    reservoir[position][j] = item\n",
    "        return reservoir\n",
    "\n",
    "    def output_samples(self, samples):\n",
    "        with open(self.output_file_path, 'w') as output_file:\n",
    "            output_file.write('ID\\tPosition\\tN\\tCov_RATIO\\n')\n",
    "            for position, sample_list in samples.items():\n",
    "                for sample in sample_list:\n",
    "                    output_file.write('\\t'.join(sample) + '\\n')\n",
    "\n",
    "    def process_files(self, folder_path):\n",
    "        file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "        stream = self.generator(file_list)\n",
    "        samples = self.reservoir_sampling(stream)\n",
    "        self.output_samples(samples)\n",
    "\n",
    "\n",
    "class AnnotDataProcessor:\n",
    "    def __init__(self, data1_file, data_folder, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def read_data1(self):\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "\n",
    "    def process_data(self):\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder_path = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/Folder_MAPLE_allPos\"\n",
    "    sampler_output = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/COV_dataProcessing_allPos.txt\"\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_allPos.txt\"\n",
    "    nb_sample = 10\n",
    "    \n",
    "    # Run the CovFileProcessor\n",
    "    cov_processor = CovFileProcessor(folder_path, checkid_file, output_folder)\n",
    "    cov_processor.run()\n",
    "\n",
    "\n",
    "    def count_files(folder_path):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise ValueError(\"Invalid folder path!\")\n",
    "\n",
    "        file_count = 0\n",
    "        for _, _, files in os.walk(folder_path):\n",
    "            file_count += len(files)\n",
    "\n",
    "        return file_count\n",
    "\n",
    "    folder_path = output_folder\n",
    "\n",
    "    try:\n",
    "        num_files = count_files(folder_path)\n",
    "        print('=========================== information =========================== ')\n",
    "        print(f\"The number of samples is: {num_files}\")\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    \n",
    "    # Run the ReservoirSampler\n",
    "    sampler = ReservoirSampler(sampler_output, nb_sample)\n",
    "    sampler.process_files(output_folder)\n",
    "    \n",
    "    # Run the AnnotDataProcessor\n",
    "    processor = AnnotDataProcessor(sampler_output, data_folder, output_file)\n",
    "    processor.write_output()\n",
    "    \n",
    "    with open(output_file, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    print('=========================== information =========================== ')\n",
    "    print(f\"The total number of samples completed for sampling is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db90d5c",
   "metadata": {},
   "source": [
    "### ERR&ALL\n",
    "```bash\n",
    "sh /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/BASH_dataProcessing_errAndall.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submit job 1\n",
    "echo \"Submitting job for OUTPUT\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forOutput.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forOutput.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forOutput.py'\n",
    "echo \"Job for output has been submitted\"\n",
    "\n",
    "# Submit job 2\n",
    "echo \"Submitting job for ANNOT\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forAnnot.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forAnnot.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forAnnot.py'\n",
    "echo \"Job for ANNOT has been submitted\"\n",
    "\n",
    "# Submit job 3\n",
    "echo \"Submitting job for COV\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forCov.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forCov.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forCov.py'\n",
    "echo \"Job for COV has been submitted\"\n",
    "\n",
    "# Submit job 4\n",
    "echo \"Submitting job for all Positions\"\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_allPos_dataProcessing.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_allPos_dataProcessing.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/CODE_allPos_dataProcessing.py'\n",
    "echo \"Job for all Positions has been submitted\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
