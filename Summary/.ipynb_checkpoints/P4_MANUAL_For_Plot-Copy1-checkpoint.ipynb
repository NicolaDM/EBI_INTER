{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a0abf7",
   "metadata": {},
   "source": [
    "## <center>Document the Data processing and visualization for Converage & Annot</center>\n",
    "##### <center>MANUAL</center>\n",
    "\n",
    "\n",
    "\n",
    "| **Label** | **start time** | **finish time** | **last modified** |\n",
    "|:--------------:|:-----------:|:-----------:|:----------------:|\n",
    "|   Project 1   |  2023-07-20 |  2023-?-? |   2023-07-20     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16cce1",
   "metadata": {},
   "source": [
    "# <center>Catalog:</center>\n",
    "- 1. [Process output](#1.Process_MAPLE_output)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 2. [Data processing for Converage](#2.Data_processing_for_Converage)\n",
    "    - [1) Decompress and save coverage](#1.Decompress_and_save_coverage)\n",
    "    - [2.1) Processing for error positions](#2.1_Processing_for_error_pos_of_Cov)\n",
    "    - [2.2) Processing for all positions](#2.2_Processing_for_all_pos_of_Cov)\n",
    "        - [2.2.1) Sampling for all positions](#2.2.1_Sampling_of_all_positions_cov)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 3. [Data processing for Annot](#3.Data_processing_for_Annot)\n",
    "    - [1) Decompress and save coverage](#1.Decompress_and_save_Annot)\n",
    "    - [2.1) Processing for error positions](#2.1_Processing_for_error_pos_of_Annot)\n",
    "    - [2.2) Processing for all positions](#2.2_Processing_for_all_pos_of_Annot)\n",
    "        - [2.2.1) Sampling for all positions](#2.2.1_Sampling_of_all_positions)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 4. [Visualization](#4.Visualization)\n",
    "    - [1) Density Chart](#1.Density_chart_(python))\n",
    "    - [2) Venn Chart](#2.Venn_chart_(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4954e7",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 3.Data_processing_for_Annot\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018e357",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.Decompress_and_save_Annot\n",
    "\n",
    "- **Idea**: \n",
    "In the old plot we were showing the frequency of the minor allele (always <50%). Now we do the same but instead of the minor allele to use the allele (nucleotide) in Colmanâ€™s assembly, which might be <50% or >50%. \n",
    "\n",
    "- **Input**: Downloaded files\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = ['/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Folder_Decompress_forP4/']\n",
    "```\n",
    "- **Address**:\n",
    "```python\n",
    "/nfs/research/goldman/zihao/N_errorsProject_1_P4/annotDecompress_andSave.py\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "Tips: All samples have now been processed and do not need to be repeated!\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_decompress/Folder_Checking/errorChecking.txt -o /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_decompress/Folder_Checking/outputChecking.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_decompress/annotDecompress_andSave.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322656b7",
   "metadata": {},
   "source": [
    "### 2\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_processing/Folder_Checking/errorChecking.txt -o /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_processing/Folder_Checking/outputChecking.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1_P4/Folder_processing/dataProcessing.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081b081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0043b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90113b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36c852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6521e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bc17b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c90a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d87d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c511fddc",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1_Processing_for_error_pos_of_Annot\n",
    "\n",
    "- **Idea**: \n",
    "    - I wrote a data processing class, DataProcessor, which has three input parameters, data1_file, data_folder and output_file. data1_file is a file containing the data to be processed. data_folder is the folder containing the other files to be looked up. output_file is the path to the file where the processed data will be output.\n",
    "\n",
    "        - In the DataProcessor class, I define three functions. read_data1 reads data from the data1_file file and returns the header of the data and the data itself. process_data processes each line of data in data1, adding two columns of data, AF and SB, by looking in the data_folder for the corresponding file. If the corresponding file is not found, the two columns are left blank. Finally, the write_output function writes the processed data to the output_file file.\n",
    "\n",
    "        - In the main function, I specify three input parameters and create an instance of the DataProcessor class. Then, I call the write_output function to process the data and output the result.\n",
    "\n",
    "    - The purpose of this class is to match and process the data in the data1_file file with the rest of the data in the data_folder and finally output the processed data to the output_file file.\n",
    "- **Input**: The file with the position of the error after processing in the [previous step](#1.Process_MAPLE_output)\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Error_pos_for_annot.txt\"]\n",
    "```\n",
    "- **Address**:\n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) Modify input path ([previous step's](#1.Process_MAPLE_output)  output)\n",
    "```bash\n",
    "sed -i 's|data1_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/output_modified.txt\"|data1_file = \"/new_path/of/output_folder/\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py\n",
    "```\n",
    "###### 2) Run the program\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/Treat_error_pos_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa8590",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2_Processing_for_all_pos_of_Annot\n",
    "\n",
    "- **Idea**: \n",
    "   - I have created two functions to process files. The first function, check_files_with_id, checks if the filenames in a given folder contain specific IDs listed in a file, and copies those files to an output folder. The second function, process_files, integrates and merges data from multiple files into a single output file for further sampling analysis.\n",
    "\n",
    "- **Input**: MapleRealErrorsVariation_errorEstimation_estimatedErrors.txt\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Annot_RATIO.txt\"]\n",
    "```\n",
    "- **Address**:\n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_all_pos.py\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) To exclude IDs that do not match the coverage data, the set of checks needs to be modified\n",
    "```bash\n",
    "sed -i 's|checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"|checkid_file = \"/new/path/to/estimatedErrors.txt\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_all_pos.py\n",
    "```\n",
    "###### 2) Modify the path of the folder where all files corresponding to the current ID are stored\n",
    "```bash\n",
    "sed -i 's|middle_output_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/PLOT_FOR_Annot/\"|middle_output_folder = \"/new/path/to/new_folder\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_all_pos.py\n",
    "```\n",
    "\n",
    "###### 3) Run the program\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/Treat_all_pos_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8083c",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### 2.2.1_Sampling_of_all_positions\n",
    "\n",
    "- **Idea**: \n",
    "    - In this code, I have defined a function called merge_files that reads two input files and writes specific rows from the second file into an output file based on matching ID and Position from the first file.\n",
    "\n",
    "    - The function starts by initializing a set called id_position_set to store unique combinations of ID and Position. Then, it reads the data file using the pandas library and iterates over each row. For each row, it combines the ID and Position values into a string and adds it to the id_position_set.\n",
    "\n",
    "    - Next, the function reads the input file line by line. It skips the title line using the next function. Then, it opens the output file for writing and writes the title line to it. For each subsequent line in the input file, it splits the line into fields using the tab separator. It extracts the ID and Position values from the fields and combines them into a string. If this string is found in the id_position_set, it writes the entire line to the output file.\n",
    "\n",
    "- **Input**: \n",
    "```python\n",
    "data_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/selected_data.txt'\n",
    "input_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Annot_RATIO.txt'\n",
    "```\n",
    "\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Annot_RATIO_sampling.txt\"]\n",
    "```\n",
    "\n",
    "- **Address**:\n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_sampling_all_pos.py\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_sampling_all_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_sampling_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b28ee",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 4.Visualization\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bdf08",
   "metadata": {},
   "source": [
    "### 1.Density_chart_(python)\n",
    "\n",
    "```python\n",
    "for_annot_AF = [\"./EBI_INTER/Project_1/Step_1.2_Annot/3_Final_plot_Annot-May.ipynb\"]\n",
    "for_coverage = [\"./EBI_INTER/Project_1/Step_1.1_Coverage/3_Final_plot_Coverage-May.ipynb\"]\n",
    "```\n",
    "\n",
    "### 2.Venn_chart_(R)\n",
    "\n",
    "```python\n",
    "# 1. Prepare the data for plotting\n",
    "dir_1 = [\"./EBI_INTER/Project_1/Step_2_Visualization_Venn/May.9_prepare_data.ipynb\"]\n",
    "# 2. Plot\n",
    "dir_2 = [\"./EBI_INTER/Project_1/Step_2_Visualization_Venn/May.2_Venn.Rmd\"]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
