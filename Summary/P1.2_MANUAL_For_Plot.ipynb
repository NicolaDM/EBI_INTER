{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a0abf7",
   "metadata": {},
   "source": [
    "## <center>Document the Data processing and visualization for Converage & Annot</center>\n",
    "##### <center>MANUAL</center>\n",
    "\n",
    "\n",
    "\n",
    "| **Label** | **start time** | **finish time** | **last modified** |\n",
    "|:--------------:|:-----------:|:-----------:|:----------------:|\n",
    "|   Project 1   |  2023-04-20 |  2023-04-24 |   2023-05-10     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16cce1",
   "metadata": {},
   "source": [
    "# <center>Catalog:</center>\n",
    "- 1. [Process output](#1.Process_MAPLE_output)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 2. [Data processing for Converage](#2.Data_processing_for_Converage)\n",
    "    - [1) Decompress and save coverage](#1.Decompress_and_save_coverage)\n",
    "    - [2.1) Processing for error positions](#2.1_Processing_for_error_pos_of_Cov)\n",
    "    - [2.2) Processing for all positions](#2.2_Processing_for_all_pos_of_Cov)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 3. [Data processing for Annot](#3.Data_processing_for_Annot)\n",
    "    - [1) Decompress and save coverage](#1.Decompress_and_save_Annot)\n",
    "    - [2.1) Processing for error positions](#2.1_Processing_for_error_pos_of_Annot)\n",
    "    - [2.2) Processing for all positions](#2.2_Processing_for_all_pos_of_Annot)\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 4. [Visualization](#4.Visualization)\n",
    "    - [1) Density Chart](#1.Density_chart_(python))\n",
    "    - [2) Venn Chart](#2.Venn_chart_(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab38fa",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.Process_MAPLE_output\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d814b",
   "metadata": {},
   "source": [
    "- **Idea**: Collate the output format and remove those with an error rate less than 0.5\n",
    "- **Input**: MapleRealErrorsVariation_errorEstimation_estimatedErrors.txt\n",
    "- **Output**: output_modified.txt\n",
    "\n",
    "### Code block:\n",
    "\n",
    "###### 1) Modify input path\n",
    "```bash\n",
    "sed -i 's|file_path = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"|file_path = \"/new_path/to/estimatedErrors.txt\"|g' /nfs/research/goldman/zihao/errorsProject_1/Handle_with_output.py\n",
    "```\n",
    "###### 2) Modify output path\n",
    "```bash\n",
    "sed -i 's|output_folder = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/\"|output_folder = \"/new_path/to/output_folder/\"|g' /nfs/research/goldman/zihao/errorsProject_1/Handle_with_output.py\n",
    "```\n",
    "###### 3) Run the program\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Handle_with_output_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Handle_with_output.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367c555",
   "metadata": {},
   "source": [
    "# 2.Data_processing_for_Converage\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc1c09",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.Decompress_and_save_coverage\n",
    "\n",
    "- **Idea**: \n",
    "    - I have defined a class called CoverageProcessor that takes two parameters: the input directory and the output directory.\n",
    "\n",
    "    - This class has two methods: get_file_paths and process_file_2. The former scans the input directory for all files ending in .coverage.gz and returns the paths to those files. The latter iterates over these files and calls the process_file method for each file.\n",
    "\n",
    "    - The process_file method processes a given .coverage.gz file, calculates the average coverage and coverage ratio for each location (calculate the **RATIO**[NB_COVERAGE/MEAN_nb_coverage]), and then writes the result to a text file.\n",
    "\n",
    "    - If the output file already exists, the process_file_2 method skips that file and continues to process the next one.\n",
    "\n",
    "    - Note that when processing a file, if any exception occurs, the file will be skipped and the program will continue to process the next file. Also, when handling an exception the program will print an error message so that we can know what went wrong.\n",
    "- **Input**: Downloaded files\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = ['/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/*_coverage.txt']\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "Tips: All samples have now been processed and do not need to be repeated!\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Coverage/Decompress_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Decompress_and_save.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e511ab",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1_Processing_for_error_pos_of_Cov\n",
    "\n",
    "- **Idea**: \n",
    "    - It is implemented through a class called DataProcessor for processing data. The constructor of the class takes three parameters: the data file path, the directory to search and the output file path.\n",
    "\n",
    "    - There are three methods in the class: search_position_value_and_get_column_4, calculate_mean_error, and process_data. The search_position_value_and_get_column_4 method is used to search for values at a given position in a file and return the fourth column of that row; the calculate_mean_error method is used to calculate the average error at each position; and the process_data method is the main method to process the data.\n",
    "\n",
    "    - The process_data method first reads the data file, stores each row of data in a list, and adds a MEAN_err column to the end of the list. Then, for each row of data, it extracts the id_now and id_position of the row and searches the specified directory for a file related to id_now. If the corresponding file is found, it calls the search_position_value_and_get_column_4 method to search for the value at the specified position and saves the result in the column. \n",
    "    \n",
    "    - Finally, at the end of the code, I create a DataProcessor instance and call the process_data method to process the given data file and write the results to the specified output file.\n",
    "- **Input**: The file with the position of the error after processing in the [previous step](#1.Process_MAPLE_output)\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Error_pos_for_coverage.txt\"]\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) Modify input path ([previous step's](#1.Process_MAPLE_output)  output)\n",
    "```bash\n",
    "sed -i 's|data_file_path = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/output_modified.txt\"|data_file_path = \"/new_path/of/output_folder/\"|g' /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_error_pos.py\n",
    "```\n",
    "###### 2) Run the program\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Coverage/Treat_error_pos_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_error_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494b4e1",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2_Processing_for_all_pos_of_Cov\n",
    "\n",
    "- **Idea**: \n",
    "   - The goal of this code is to handle data in files and folders. It has two main functions.\n",
    "\n",
    "   - The first function check_files_with_id is used to check if the file names in a folder contain the IDs in the specified files, and to copy the files that meet the criteria into the output folder.\n",
    "\n",
    "   - The second function process_files is used to consolidate and merge data from multiple files in a folder into a new text file.\n",
    "\n",
    "   - With these functions, we can easily filter files and consolidate data to meet specific needs.\n",
    "\n",
    "- **Input**: MapleRealErrorsVariation_errorEstimation_estimatedErrors.txt\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Cov_RATIO.txt\"]\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) To exclude IDs that do not match the coverage data, the set of checks needs to be modified\n",
    "```bash\n",
    "sed -i 's|checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"|checkid_file = \"/new/path/to/estimatedErrors.txt\"|g' /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_all_pos.py\n",
    "```\n",
    "###### 2) Modify the path of the folder where all files corresponding to the current ID are stored\n",
    "```bash\n",
    "sed -i 's|middle_output_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/PLOT_FOR_Coverage/\"|middle_output_folder = \"/new/path/to/new_folder\"|g' /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_all_pos.py\n",
    "```\n",
    "\n",
    "###### 3) Run the program\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Coverage/Treat_all_pos_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4954e7",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 3.Data_processing_for_Annot\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018e357",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.Decompress_and_save_Annot\n",
    "\n",
    "- **Idea**: \n",
    "    - To process VCF files, I created a Python class called VCFProcessor. This class contains two attributes that represent the input and output directories, respectively: input_dir and output_dir.\n",
    "\n",
    "        - I define the static function parse_info_field, which parses the VCF file's INFO field and converts it to a Python dictionary.\n",
    "\n",
    "        - I also defined the process_vcf_file method, which processes a single VCF file and returns the processed data. Using Python's csv module, I first read the file and parse it. The parse_info_field method is then used to parse the INFO fields and retrieve the essential information such as chromosome position, reference and substitution bases, mutation frequency, and so on. Following that, I filter out INDEL, convert the mutation frequency to a decimal number, and check to see if it is larger than 0.5. If this is the case, I change it to 1 minus its value. Finally, I return the processed data to a dictionary.\n",
    "\n",
    "        - I also define the process_files method, which will process all of the VCF files in the input directory. In this function, I utilise the glob module to retrieve all of the file paths that satisfy the criteria and then use the process_vcf_file method on each file individually to process it. If the processing is successful, the result is saved to the output directory as a TXT file.\n",
    "\n",
    "    - I define a variable length_of_sample in the main function to indicate the length of the sample. Then I define the paths to the input and output directories and construct a VCFProcessor object. Finally, I use the process_files method to go through all of the VCF files in the input directory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Input**: Downloaded files\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = ['/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Decompress/*_annot.txt']\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "Tips: All samples have now been processed and do not need to be repeated!\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_decompress_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Decompress_and_save.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511fddc",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.1_Processing_for_error_pos_of_Annot\n",
    "\n",
    "- **Idea**: \n",
    "    - I wrote a data processing class, DataProcessor, which has three input parameters, data1_file, data_folder and output_file. data1_file is a file containing the data to be processed. data_folder is the folder containing the other files to be looked up. output_file is the path to the file where the processed data will be output.\n",
    "\n",
    "        - In the DataProcessor class, I define three functions. read_data1 reads data from the data1_file file and returns the header of the data and the data itself. process_data processes each line of data in data1, adding two columns of data, AF and SB, by looking in the data_folder for the corresponding file. If the corresponding file is not found, the two columns are left blank. Finally, the write_output function writes the processed data to the output_file file.\n",
    "\n",
    "        - In the main function, I specify three input parameters and create an instance of the DataProcessor class. Then, I call the write_output function to process the data and output the result.\n",
    "\n",
    "    - The purpose of this class is to match and process the data in the data1_file file with the rest of the data in the data_folder and finally output the processed data to the output_file file.\n",
    "- **Input**: The file with the position of the error after processing in the [previous step](#1.Process_MAPLE_output)\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Error_pos_for_annot.txt\"]\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) Modify input path ([previous step's](#1.Process_MAPLE_output)  output)\n",
    "```bash\n",
    "sed -i 's|data1_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/output_modified.txt\"|data1_file = \"/new_path/of/output_folder/\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py\n",
    "```\n",
    "###### 2) Run the program\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Annot/Treat_error_pos_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8083c",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2_Processing_for_all_pos_of_Annot\n",
    "\n",
    "- **Idea**: \n",
    "    - In this code, I created a class RatioProcessor to compute the maximum ratio values across all files in a folder for two different types of ratios (SB and AF). The output is written to separate text files for each ratio type.\n",
    "\n",
    "    - The check_files_with_id function is called first to copy files in the given folder that contain specified IDs in a separate output folder. Then, in the RatioProcessor class, the get_files method returns a list of all files in the folder. The read_ratios method reads the ratio values from the file and the write_ratios method writes the header and ratio values to the output file. The process_files method computes the max ratio values for the given ratio type across all files using numpy.max. Finally, in the run method, the process_files method is called for both ratio types, and the results are written to separate output files in the specified output folder.\n",
    "\n",
    "- **Input**: MapleRealErrorsVariation_errorEstimation_estimatedErrors.txt\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = [\"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Cov_RATIO.txt\"]\n",
    "```\n",
    "\n",
    "##### Code block:\n",
    "###### 1) To exclude IDs that do not match the coverage data, the set of checks needs to be modified\n",
    "```bash\n",
    "sed -i 's|checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"|checkid_file = \"/new/path/to/estimatedErrors.txt\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py\n",
    "```\n",
    "###### 2) Modify the path of the folder where all files corresponding to the current ID are stored\n",
    "```bash\n",
    "sed -i 's|middle_output_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/PLOT_FOR_Annot/\"|middle_output_folder = \"/new/path/to/new_folder\"|g' /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py\n",
    "```\n",
    "\n",
    "###### 3) Run the program\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/Annot/Treat_error_pos_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_error_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b28ee",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 4.Visualization\n",
    "[Return to Catalog](#Catalog:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2bdf08",
   "metadata": {},
   "source": [
    "### 1.Density_chart_(python)\n",
    "\n",
    "```python\n",
    "for_annot_AF = [\"./EBI_INTER/Project_1/Step_1.2_Annot/3_Final_plot_Annot-May.ipynb\"]\n",
    "for_coverage = [\"./EBI_INTER/Project_1/Step_1.1_Coverage/3_Final_plot_Coverage-May.ipynb\"]\n",
    "```\n",
    "\n",
    "### 2.Venn_chart_(R)\n",
    "\n",
    "```python\n",
    "# 1. Prepare the data for plotting\n",
    "dir_1 = [\"./EBI_INTER/Project_1/Step_2_Visualization_Venn/May.9_prepare_data.ipynb\"]\n",
    "# 2. Plot\n",
    "dir_2 = [\"./EBI_INTER/Project_1/Step_2_Visualization_Venn/May.2_Venn.Rmd\"]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
