{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfc1c09",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcd19",
   "metadata": {},
   "source": [
    "## 1.1 Data slicing (for easy download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599b741",
   "metadata": {},
   "source": [
    "- **Idea**: cut all files into 100 copies so that they can be downloaded at the same time\n",
    "- **Input**:\n",
    "```python\n",
    "input_files = ['/nfs/research/goldman/zihao/Datas/p1/File_5_Annot.txt',\n",
    "               '/nfs/research/goldman/zihao/Datas/p1/File_5_Consensus.txt',\n",
    "               '/nfs/research/goldman/zihao/Datas/p1/File_5_Coverage.txt']\n",
    "```\n",
    "- **Output**:\n",
    "```python\n",
    "output_directories = ['/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Datas/batch*.txt',\n",
    "                      '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Datas/batch*.txt',\n",
    "                      '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Datas/batch*.txt']\n",
    "```\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/1_Download/Data_slicing.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87ab6a",
   "metadata": {},
   "source": [
    "```python\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the paths for the input files and output directories\n",
    "input_files = ['/nfs/research/goldman/zihao/Datas/p1/File_5_Annot.txt',\n",
    "               '/nfs/research/goldman/zihao/Datas/p1/File_5_Consensus.txt',\n",
    "               '/nfs/research/goldman/zihao/Datas/p1/File_5_Coverage.txt']\n",
    "output_directories = ['/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Datas',\n",
    "                      '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Datas',\n",
    "                      '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Datas']\n",
    "\n",
    "# Define the batch size for each file\n",
    "batch_size = 100\n",
    "\n",
    "# Loop through the input files and split them into batches\n",
    "for input_file, output_directory in zip(input_files, output_directories):\n",
    "    with open(input_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        header = next(reader)\n",
    "        rows = [row for row in reader]\n",
    "\n",
    "    num_batches = len(rows) // batch_size + 1\n",
    "    for i, batch in enumerate(range(0, len(rows), batch_size)):\n",
    "        filename = f\"batch_{i+1}.txt\"\n",
    "        batch_rows = rows[batch:batch+batch_size]\n",
    "        with open(os.path.join(output_directory, filename), 'w', newline='') as f:\n",
    "            writer = csv.writer(f, delimiter='\\t')\n",
    "            writer.writerow(header)\n",
    "            writer.writerows(batch_rows)\n",
    "        del batch_rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659a16c",
   "metadata": {},
   "source": [
    "***\n",
    "## 1.2 Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64898b3",
   "metadata": {},
   "source": [
    "- **Idea**: Download the corresponding sample files from the files cut in the previous steps while following the FTP link\n",
    "\n",
    "\n",
    "- **Input&Output**:\n",
    "```python\n",
    "for use_case in \"File_5_annot\" \"File_5_consensus\" \"File_5_coverage\"; do\n",
    "      input_folder_path=\"$input_base_path/$use_case/Datas\"\n",
    "      output_folder_path=\"$output_base_path/$use_case/Downloads\"\n",
    "```\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/1_Download/run_data_download.sh\n",
    "/nfs/research/goldman/zihao/errorsProject_1/1_Download/D_file_5_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98cc41",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "cd /nfs/research/goldman/zihao/errorsProject_1/1_Download\n",
    "\n",
    "# Define the common parts of the input and output folder paths\n",
    "input_base_path=\"/nfs/research/goldman/zihao/Datas/p1\"\n",
    "output_base_path=\"/nfs/research/goldman/zihao/Datas/p1\"\n",
    "\n",
    "# Loop over the different use cases\n",
    "for use_case in \"File_5_annot\" \"File_5_consensus\" \"File_5_coverage\"; do\n",
    "  input_folder_path=\"$input_base_path/$use_case/Datas\"\n",
    "  output_folder_path=\"$output_base_path/$use_case/Downloads\"\n",
    "\n",
    "  # Loop over the batches\n",
    "  for i in {1..100}; do\n",
    "    input_file=\"batch_${i}.txt\"\n",
    "    \n",
    "    # Call the script with bsub command\n",
    "    bsub -M 2000 python3 D_file_5_script.py -i \"$input_folder_path/$input_file\" -o \"$output_folder_path\"\n",
    "  done\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16449f8b",
   "metadata": {},
   "source": [
    "```python\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import urllib.request\n",
    "import gc\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Download files from FTP path')\n",
    "parser.add_argument('--input-txt', '-i', required=True, help='Path to the input txt file')\n",
    "parser.add_argument('--output-folder', '-o', required=True, help='Path to the output folder to save the downloaded files')\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_file_path = args.input_txt\n",
    "output_folder_path = args.output_folder\n",
    "\n",
    "\n",
    "def download_files_from_ftp(input_file_path, output_folder_path, max_retries=5):\n",
    "    with open(input_file_path, 'r') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        files_downloaded = False\n",
    "        download_attempts = 0\n",
    "        while not files_downloaded and download_attempts < 10:\n",
    "            files_downloaded = True\n",
    "            download_attempts += 1\n",
    "            for row in reader:\n",
    "                ftp_path = row['FTP_path']\n",
    "                file_name = ftp_path.split('/')[-1]\n",
    "                file_path = os.path.join(output_folder_path, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    continue\n",
    "                for i in range(max_retries):\n",
    "                    try:\n",
    "                        urllib.request.urlretrieve(ftp_path, file_path)\n",
    "                        break\n",
    "                    except:\n",
    "                        if i == max_retries - 1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            continue\n",
    "                if not os.path.exists(file_path):\n",
    "                    files_downloaded = False\n",
    "                    break\n",
    "            # Force garbage collection at the end of each loop iteration\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "download_files_from_ftp(input_file_path, output_folder_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43b76a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 2. Data processing (adjusting sequence format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fa076",
   "metadata": {},
   "source": [
    "1. Put all the downloaded consensus sequences in a single fasta file (let’s call it all_consensuses.fasta ).\n",
    "    \n",
    "2. Run mafft with the special options we mentioned before including --keeplength and using the reference I sent you MN908947.3 , let’s call the output all_consensuses_aligned.fasta .\n",
    "    \n",
    "3. Remove the reference sequence from the mafft alignment output (it should be the first sequence in the file), let’s call the resulting file all_consensuses_aligned_noReference.fasta .\n",
    "    \n",
    "4. Run my script createMapleFile.py on  all_consensuses_aligned_noReference.fasta WITHOUT using option --reference . This will create a MAPLE file for you, and a new reference to use with MAPLE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a6ba6",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.1. Decompress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176d20a",
   "metadata": {},
   "source": [
    "- **Idea**: Decompress the downloaded file\n",
    "- **Input**: All files downloaded in the previous step\n",
    "\n",
    "- **Output**: \n",
    "    - all_consensuses_batch_*.fasta\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Consensuses/Decompress.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1c817",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "# Define the input folder and output folder to be used\n",
    "input_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Downloads/'\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/'\n",
    "\n",
    "# List all gzip files in the input folder\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith('.gz')]\n",
    "\n",
    "# Calculate the number of files to be written\n",
    "num_files = 20\n",
    "\n",
    "# Calculate the number of files in each batch\n",
    "files_per_batch = (len(file_list) + num_files - 1) // num_files\n",
    "\n",
    "# Process files in batches\n",
    "for i in range(num_files):\n",
    "    # Get the list of files for the current batch\n",
    "    batch_files = file_list[i*files_per_batch : (i+1)*files_per_batch]\n",
    "    \n",
    "    # Create an output file to store the fasta sequences of all sequences in the current batch\n",
    "    output_file = os.path.join(output_folder, f\"all_consensuses_batch_{i+1}.fasta\")\n",
    "    \n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        # Iterate over the files in the current batch\n",
    "        for file_name in batch_files:\n",
    "            \n",
    "            # Get the sequence name\n",
    "            seq_name = file_name.split('_')[0]\n",
    "\n",
    "            # Check if the file exists\n",
    "            if not os.path.isfile(os.path.join(input_folder, file_name)):\n",
    "                print(f\"File {file_name} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Open the compressed file\n",
    "            with gzip.open(os.path.join(input_folder, file_name), \"rt\") as f:\n",
    "\n",
    "                # Use the SeqIO module to parse the fasta file\n",
    "                sequences = SeqIO.parse(f, \"fasta\")\n",
    "\n",
    "                # Write the fasta sequences to the output file\n",
    "                for seq_record in sequences:\n",
    "                    # Modify the sequence name to be the file name\n",
    "                    seq_record.id = seq_name\n",
    "                    seq_record.description = \"\"\n",
    "                    SeqIO.write(seq_record, out_file, \"fasta\")\n",
    "    \n",
    "    print(f\"Completed processing batch {i+1} of files. Output file: {os.path.basename(output_file)}\")\n",
    "    \n",
    "    # Free up memory\n",
    "    del batch_files\n",
    "    del out_file\n",
    "    del sequences\n",
    "    del seq_record\n",
    "    del f\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def89d6",
   "metadata": {},
   "source": [
    "### 2.1.2 Sequence Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b90f2",
   "metadata": {},
   "source": [
    "- **Idea**: Alignment using MAFFT software and according to MN908947.3 as reference\n",
    "\n",
    "- **Input**: All files decompressed in the previous step\n",
    "\n",
    "- **Output**: \n",
    "    - /nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/aligned_*.fasta\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Consensuses/Aligned.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6fc5e",
   "metadata": {},
   "source": [
    "##### Code block:\n",
    "```bash\n",
    "cd /nfs/research/goldman/zihao/errorsProject_1/Consensuses\n",
    "sh Aligned.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45261aa0",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "for i in {1..20}\n",
    "do\n",
    "  mafft --6merpair --keeplength --addfragments all_consensuses_$i.fasta ref_MN908947.3.fasta > all_consensuses_aligned_$i.fasta\n",
    "done\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051963",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.2. Del ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1fdb1",
   "metadata": {},
   "source": [
    "- **Idea**: Delete reference MN908947.3\n",
    "\n",
    "- **Input**: All files aligned in the previous step\n",
    "\n",
    "- **Output**: \n",
    "    - all_consensuses_aligned_noReference_aligned_*.fasta\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Consensuses/Del_ref.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8bb94b",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "# Define the input folder and output folder to be used\n",
    "input_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/'\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/'\n",
    "\n",
    "# Traverse all fasta files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.fasta'):\n",
    "        # Create the output file name\n",
    "        output_file = os.path.join(output_folder, 'all_consensuses_aligned_noReference_' + filename[:-6] + '.fasta')\n",
    "        if not os.path.exists(output_file):\n",
    "            open(output_file, 'w').close()\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        # Read the file content\n",
    "        with open(input_file) as f:\n",
    "            content = f.read()\n",
    "        # Find the first occurrence of '>MN908947.3'\n",
    "        start_index = content.find('>MN908947.3')\n",
    "        # Find the next occurrence of '>'\n",
    "        end_index = content.find('>', start_index + 1)\n",
    "        # Remove the content within the specified range\n",
    "        content = content[:start_index] + content[end_index:]\n",
    "        # Check if there are still headers with '>MN908947.3'\n",
    "        while '>MN908947.3' in content:\n",
    "            start_index = content.find('>MN908947.3')\n",
    "            end_index = content.find('>', start_index + 1)\n",
    "            if end_index == -1:\n",
    "                print('The header for the next sequence is not found in the file content')\n",
    "                break\n",
    "            else:\n",
    "                next_header = content[end_index:].split('\\n', 1)[0]\n",
    "                print(f\"The header for the next sequence is: {next_header}\")\n",
    "                content = content[:start_index] + content[end_index:]\n",
    "        # Write the processed content to the output file\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(content)\n",
    "        # Delete the content variable\n",
    "        del content\n",
    "        # Print a success message\n",
    "        print(f\"{filename} has been processed and saved to {os.path.basename(output_file)}\")\n",
    "        \n",
    "print('Delete is completed')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e452980",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.3. Merge + Remove blank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52f5d1",
   "metadata": {},
   "source": [
    "- Input: all_consensuses_aligned_noReference_aligned_*.fasta\n",
    "\n",
    "- Output: \n",
    "    - After merge: all_consensuses_aligned_noReference.fasta\n",
    "\n",
    "    - After Remove: rm_blank_all_consensuses_aligned_noReference.fasta\n",
    "- **Location**： \n",
    "```python\n",
    "/nfs/research/goldman/zihao/errorsProject_1/Consensuses/Merge_Remove_blank.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabfe3e",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "import glob\n",
    "\n",
    "# Define the input folder path and output file path for concatenation\n",
    "concat_input_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference'\n",
    "concat_output_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/all_consensuses_aligned_noReference.fasta'\n",
    "\n",
    "# Get a list of all input file paths that start with 'all_consensuses_aligned_noReference_aligned_'\n",
    "concat_input_files = glob.glob(os.path.join(concat_input_folder, 'all_consensuses_aligned_noReference_aligned_*.fasta'))\n",
    "\n",
    "# Concatenate all input files into a single file\n",
    "with open(concat_output_file, 'w') as concat_f_out:\n",
    "    for concat_input_file in concat_input_files:\n",
    "        with open(concat_input_file) as concat_f_in:\n",
    "            concat_f_out.write(concat_f_in.read())\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"All input files starting with 'all_consensuses_aligned_noReference_aligned_' have been merged and saved to {os.path.basename(concat_output_file)}\")\n",
    "\n",
    "\n",
    "# Define the input and output file paths for sequence filtering\n",
    "filter_input_file = \"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/all_consensuses_aligned_noReference.fasta\"\n",
    "filter_output_file = \"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/rm_blank_all_consensuses_aligned_noReference.fasta\"\n",
    "removed_file = \"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/blank_bigger_than_20.fasta\"\n",
    "\n",
    "removed_count = 0\n",
    "\n",
    "# Filter sequences based on the percentage of gaps and N's\n",
    "with open(filter_input_file, \"rU\") as filter_input_handle, open(filter_output_file, \"w\") as filter_output_handle, open(removed_file, \"w\") as removed_handle:\n",
    "    for record in SeqIO.parse(filter_input_handle, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        n_count = sequence.count(\"n\") + sequence.count(\"N\")\n",
    "        gap_count = sequence.count(\"-\")\n",
    "        total_count = n_count + gap_count\n",
    "        if total_count/len(sequence) <= 0.2:\n",
    "            SeqIO.write(record, filter_output_handle, \"fasta\")\n",
    "        else:\n",
    "            print(f\"{record.id} has {total_count} gaps or N's and is being removed.\")\n",
    "            removed_handle.write(record.format(\"fasta\"))\n",
    "            removed_count += 1\n",
    "\n",
    "print(f\"Removed {removed_count} sequences.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4061f",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.4. Transform as the MAPLE format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b205bc6a",
   "metadata": {},
   "source": [
    "Run the script createMapleFile.py on  all_consensuses_aligned_noReference.fasta WITHOUT using option --reference . This will create a MAPLE file for you, and a new reference to use with MAPLE.\n",
    "\n",
    "##### Code block:\n",
    "```bash\n",
    "bsub \"/hps/software/users/goldman/pypy3/pypy3.7-v7.3.5-linux64/bin/pypy3 \n",
    "createMapleFile.py \n",
    "--path /nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/ \n",
    "--fasta rm_blank_all_consensuses_aligned_noReference.fasta \n",
    "--output MAPLE_format_consensuses.txt\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e21978b",
   "metadata": {},
   "source": [
    "### Explanation of MAPLE format\n",
    "#### > SRR20944325\n",
    "\n",
    "|         |         |         |\n",
    "|---------|---------|---------|\n",
    "| - | 1 | 2 |\n",
    "| n | 3 | 29901 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58922471",
   "metadata": {},
   "source": [
    "- The second column number tells you the genome position the entry refers to.\n",
    "\n",
    "- The third column\n",
    "    - If the third column is not present, then the entry refers to only one position.\n",
    "    - If the third column is present, its value tells you how many positions that entry refers to\n",
    "\n",
    "- For example\n",
    "|         |         |         |\n",
    "|---------|---------|---------|\n",
    "| n | 541 | 10 |\n",
    "\n",
    "    - means that “n” is present in the sequence in ten consecutive positions from position 541 up to position 550."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702081d1",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# 3. Run MAPLE program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8414de4",
   "metadata": {},
   "source": [
    "##### Code block:\n",
    "```bash\n",
    "bsub -g /MapleRealErrors -q long -M 40000 \n",
    "-o /nfs/research/goldman/zihao/errorsProject_1/MAPLE/MAPLE_realData_errorChecking_output.txt \n",
    "-e /nfs/research/goldman/zihao/errorsProject_1/MAPLE/MAPLE_realData_errorChecking_error.txt \n",
    "/hps/software/users/goldman/pypy3/pypy3.7-v7.3.5-linux64/bin/pypy3 \n",
    "/nfs/research/goldman/demaio/fastLK/code/MAPLEv0.3.2.py \n",
    "--model UNREST --rateVariation --estimateSiteSpecificErrorRate \n",
    "--input /nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned/noReference/MAPLE_format_consensuses_new_1.txt \n",
    "--overwrite --estimateErrors --calculateLKfinalTree \n",
    "--output /nfs/research/goldman/zihao/errorsProject_1/MAPLE/MAPLE0.3.1_rateVar_errors_realData_checkingErrors\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
