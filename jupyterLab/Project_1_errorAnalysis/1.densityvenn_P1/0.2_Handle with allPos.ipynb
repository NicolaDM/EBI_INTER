{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ade10fa",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Combine to three classes\n",
    "2. Excuate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6c1e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136344"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/nfs/research/goldman/zihao/Datas/p2_compViridian_P2/Folder_1_dataPrep/Folder_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "data_set = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            data_set.add(line.strip())\n",
    "\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1a61",
   "metadata": {},
   "source": [
    "## 1.forCoverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0db351",
   "metadata": {},
   "source": [
    "## 2.sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d2eda",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/TEST/errorChecking.txt -o /nfs/research/goldman/zihao/TEST/outputChecking.txt 'python3 /nfs/research/goldman/zihao/TEST/allPos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf59a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c94f3c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Position</th>\n",
       "      <th>N</th>\n",
       "      <th>Cov_RATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERR6542228</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SRR23130330</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ERR7897214</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ERR6499101</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ERR4905438</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ERR6483047</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ERR7897214</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ERR6524376</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SRR20724513</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ERR6682766</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ERR7897214</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SRR23130330</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ERR8018356</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ERR7844051</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SRR21793936</td>\n",
       "      <td>3</td>\n",
       "      <td>T</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SRR21719453</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ERR7897214</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ERR4905438</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0.002251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ERR6682766</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ERR6524376</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Position   N  Cov_RATIO\n",
       "0    ERR6542228         1  A    0.000000\n",
       "1   SRR23130330         1  A    0.000000\n",
       "2    ERR7897214         1  A    0.000000\n",
       "3    ERR6499101         1  A    0.000000\n",
       "4    ERR4905438         1  A    0.000225\n",
       "5    ERR6483047         2  T    0.000214\n",
       "6    ERR7897214         2  T    0.000000\n",
       "7    ERR6524376         2  T    0.000000\n",
       "8   SRR20724513         2  T    0.000000\n",
       "9    ERR6682766         2  T    0.000000\n",
       "10   ERR7897214         3  T    0.000000\n",
       "11  SRR23130330         3  T    0.000595\n",
       "12   ERR8018356         3  T    0.000000\n",
       "13   ERR7844051         3  T    0.000000\n",
       "14  SRR21793936         3  T    0.000000\n",
       "15  SRR21719453         4  A    0.000000\n",
       "16   ERR7897214         4  A    0.000000\n",
       "17   ERR4905438         4  A    0.002251\n",
       "18   ERR6682766         4  A    0.001064\n",
       "19   ERR6524376         4  A    0.000730"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('N-dataProcessing_allPos.txt',sep = '\\t').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "302761f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.214893341064453\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, max_copy=50):\n",
    "    id_set = set()\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "    count = 0\n",
    "    selected_files = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            selected_files.append(os.path.join(folder_path, filename))\n",
    "            count += 1\n",
    "        if count >= max_copy:\n",
    "            break\n",
    "    return selected_files\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def generator(self, file_name):\n",
    "        with open(file_name, 'r') as file:\n",
    "            file_id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            file_id = file_id.replace('_coverage', '')\n",
    "            next(file)\n",
    "            for line in file:\n",
    "                yield [file_id] + line.strip().split('\\t')\n",
    "\n",
    "    def reservoir_sampling(self, stream):\n",
    "        reservoir = {}\n",
    "        for item in stream:\n",
    "            position = int(item[1])\n",
    "            if position not in reservoir:\n",
    "                reservoir[position] = [item]\n",
    "            elif len(reservoir[position]) < self.k:\n",
    "                reservoir[position].append(item)\n",
    "            else:\n",
    "                j = random.randint(0, len(reservoir[position]) + 1)\n",
    "                if j < self.k:\n",
    "                    reservoir[position][j] = item\n",
    "        return reservoir\n",
    "\n",
    "def process_file(file_name):\n",
    "    sampler = ReservoirSampler(5)\n",
    "    stream = sampler.generator(file_name)\n",
    "    samples = sampler.reservoir_sampling(stream)\n",
    "    return samples\n",
    "\n",
    "def merge_samples(all_samples_list, k):\n",
    "    all_samples = {}\n",
    "    for samples in all_samples_list:\n",
    "        for position, sample_list in samples.items():\n",
    "            if position not in all_samples:\n",
    "                all_samples[position] = sample_list\n",
    "            else:\n",
    "                all_samples[position].extend(sample_list)\n",
    "                if len(all_samples[position]) > k:\n",
    "                    random.shuffle(all_samples[position])\n",
    "                    all_samples[position] = all_samples[position][:k]\n",
    "    return all_samples\n",
    "\n",
    "def main():\n",
    "    folder_path_1 = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_compViridian_P2/Folder_1_dataPrep/Folder_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "    output_file_path = \"N-dataProcessing_allPos.txt\"\n",
    "\n",
    "    selected_files = check_files_with_id(folder_path_1, checkid_file)\n",
    "\n",
    "    with Pool() as pool:\n",
    "        all_samples_list = pool.map(process_file, selected_files)\n",
    "\n",
    "    all_samples = merge_samples(all_samples_list, 5)\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.write('ID\\tPosition\\tN\\tCov_RATIO\\n')\n",
    "        for position, sample_list in all_samples.items():\n",
    "            for sample in sample_list:\n",
    "                output_file.write('\\t'.join(sample) + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c68d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bb0ec84",
   "metadata": {},
   "source": [
    "## 3.forAnnot【ing】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, max_copy=50):\n",
    "    id_set = set()\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "    count = 0\n",
    "    selected_files = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            selected_files.append(os.path.join(folder_path, filename))\n",
    "            count += 1\n",
    "        if count >= max_copy:\n",
    "            break\n",
    "    return selected_files\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def generator(self, file_name):\n",
    "        with open(file_name, 'r') as file:\n",
    "            file_id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "            file_id = file_id.replace('_coverage', '')\n",
    "            next(file)\n",
    "            for line in file:\n",
    "                yield [file_id] + line.strip().split('\\t')\n",
    "\n",
    "    def reservoir_sampling(self, stream):\n",
    "        reservoir = {}\n",
    "        for item in stream:\n",
    "            position = int(item[1])\n",
    "            if position not in reservoir:\n",
    "                reservoir[position] = [item]\n",
    "            elif len(reservoir[position]) < self.k:\n",
    "                reservoir[position].append(item)\n",
    "            else:\n",
    "                j = random.randint(0, len(reservoir[position]) + 1)\n",
    "                if j < self.k:\n",
    "                    reservoir[position][j] = item\n",
    "        return reservoir\n",
    "\n",
    "def process_file(file_name):\n",
    "    sampler = ReservoirSampler(5)\n",
    "    stream = sampler.generator(file_name)\n",
    "    samples = sampler.reservoir_sampling(stream)\n",
    "    return samples\n",
    "\n",
    "def merge_samples(all_samples_list, k):\n",
    "    all_samples = {}\n",
    "    for samples in all_samples_list:\n",
    "        for position, sample_list in samples.items():\n",
    "            if position not in all_samples:\n",
    "                all_samples[position] = sample_list\n",
    "            else:\n",
    "                all_samples[position].extend(sample_list)\n",
    "                if len(all_samples[position]) > k:\n",
    "                    random.shuffle(all_samples[position])\n",
    "                    all_samples[position] = all_samples[position][:k]\n",
    "    return all_samples\n",
    "\n",
    "def main():\n",
    "    folder_path_1 = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_compViridian_P2/Folder_1_dataPrep/Folder_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "\n",
    "    selected_files = check_files_with_id(folder_path_1, checkid_file)\n",
    "\n",
    "    with Pool() as pool:\n",
    "        all_samples_list = pool.map(process_file, selected_files)\n",
    "\n",
    "    all_samples = merge_samples(all_samples_list, 5)\n",
    "\n",
    "    # Pass the all_samples data to the DataProcessor\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"ANNOT_dataProcessing_allPos.txt\"\n",
    "    #\"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_allPos.txt\"\n",
    "\n",
    "    processor = DataProcessor(all_samples, data_folder, output_file)\n",
    "    processor.write_output()\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data1, data_folder, output_file):\n",
    "        self.data1 = data1\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "    \n",
    "    def read_data1(self):\n",
    "        data1 = []\n",
    "        for position, sample_list in self.data1.items():\n",
    "            for sample in sample_list:\n",
    "                # Ensure all elements in sample are strings\n",
    "                data1.append([str(element) for element in sample])\n",
    "        header = 'ID\\tPosition\\tN\\tCov_RATIO\\tAF\\tSB\\n'\n",
    "        return header, data1\n",
    "    \n",
    "    def process_data(self):\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        # Convert row[1] to string for comparison\n",
    "                        if line[0] == str(row[1]):\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        \"\"\"Write the processed data1 to the output file.\"\"\"\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3695a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7763a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, data1_file, data_folder, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "    \n",
    "    def read_data1(self):\n",
    "        \"\"\"Read data1 from file and return the header and data.\"\"\"\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "    \n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Process data1 by looking up corresponding rows in data folder.\"\"\"\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        \"\"\"Write the processed data1 to the output file.\"\"\"\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "if __name__ == '__main__':\n",
    "    data1_file = sampler_output\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_allPos.txt\"\n",
    "\n",
    "    processor = DataProcessor(data1_file, data_folder, output_file)\n",
    "    processor.write_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81089ea1",
   "metadata": {},
   "source": [
    "```python\n",
    "def read_data1(self):\n",
    "    \"\"\"Read data1 from file and return the header and data.\"\"\"\n",
    "    data1 = []\n",
    "    with open(self.data1_file) as f:\n",
    "        header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "        count = 0  # 计数器\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            data1.append(line)\n",
    "            count += 1\n",
    "            if count == 10:  # 达到10行后终止循环\n",
    "                break\n",
    "    return header, data1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb78d3",
   "metadata": {},
   "source": [
    "#### Sampler [Unencapsulate]\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "\n",
    "def generator(file_list):\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, 'r') as file:\n",
    "            next(file)  # skip the header line\n",
    "            for line in file:\n",
    "                yield line.strip().split('\\t')  # split by tab\n",
    "\n",
    "def reservoir_sampling(stream, k):\n",
    "    # Initialize the reservoir for each position\n",
    "    reservoir = {}\n",
    "    for item in stream:\n",
    "        position = int(item[0])  # Position is in the first column\n",
    "        if position not in reservoir:\n",
    "            reservoir[position] = [item]\n",
    "        elif len(reservoir[position]) < k:\n",
    "            reservoir[position].append(item)\n",
    "        else:\n",
    "            # Replace elements with gradually decreasing probability\n",
    "            j = random.randint(0, len(reservoir[position]) + 1)\n",
    "            if j < k:\n",
    "                reservoir[position][j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Get a list of all the files in the directory\n",
    "folder_path = output_folder\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "\n",
    "# Create the generator\n",
    "stream = generator(file_list)\n",
    "\n",
    "# Reservoir sampling\n",
    "k = 5  # The number of items to sample for each position\n",
    "samples = reservoir_sampling(stream, k)\n",
    "\n",
    "# Output the samples to a file\n",
    "with open('/nfs/research/goldman/zihao/code/output.txt', 'w') as output_file:\n",
    "    for position, sample_list in samples.items():\n",
    "        for sample in sample_list:\n",
    "            output_file.write('\\t'.join(sample) + '\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe38537",
   "metadata": {},
   "source": [
    "### [!!!] Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d59a57",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/Folder_Checking/errorChecking_allPos_dataProcessing.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/Folder_Checking/outputChecking_allPos_dataProcessing.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/CODE_allPos_dataProcessing.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "class CovFileProcessor:\n",
    "    def __init__(self, folder_path, checkid_file, output_folder):\n",
    "        self.folder_path = folder_path\n",
    "        self.checkid_file = checkid_file\n",
    "        self.output_folder = output_folder\n",
    "\n",
    "    def run(self):\n",
    "        id_set = set()\n",
    "\n",
    "        with open(self.checkid_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    id_set.add(line[1:])\n",
    "\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "        for filename in os.listdir(self.folder_path):\n",
    "            if any(id_str in filename for id_str in id_set):\n",
    "                shutil.copy(os.path.join(self.folder_path, filename), os.path.join(self.output_folder, filename))\n",
    "\n",
    "\n",
    "class ReservoirSampler:\n",
    "    def __init__(self, output_file_path, k):\n",
    "        self.output_file_path = output_file_path\n",
    "        self.k = k\n",
    "\n",
    "    def generator(self, file_list):\n",
    "        for file_name in file_list:\n",
    "            with open(file_name, 'r') as file:\n",
    "                file_id = os.path.splitext(os.path.basename(file_name))[0]\n",
    "                file_id = file_id.replace('_coverage', '')\n",
    "                next(file)\n",
    "                for line in file:\n",
    "                    yield [file_id] + line.strip().split('\\t')\n",
    "\n",
    "    def reservoir_sampling(self, stream):\n",
    "        reservoir = {}\n",
    "        for item in stream:\n",
    "            position = int(item[1])\n",
    "            if position not in reservoir:\n",
    "                reservoir[position] = [item]\n",
    "            elif len(reservoir[position]) < self.k:\n",
    "                reservoir[position].append(item)\n",
    "            else:\n",
    "                j = random.randint(0, len(reservoir[position]) + 1)\n",
    "                if j < self.k:\n",
    "                    reservoir[position][j] = item\n",
    "        return reservoir\n",
    "\n",
    "    def output_samples(self, samples):\n",
    "        with open(self.output_file_path, 'w') as output_file:\n",
    "            output_file.write('ID\\tPosition\\tN\\tCov_RATIO\\n')\n",
    "            for position, sample_list in samples.items():\n",
    "                for sample in sample_list:\n",
    "                    output_file.write('\\t'.join(sample) + '\\n')\n",
    "\n",
    "    def process_files(self, folder_path):\n",
    "        file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".txt\")]\n",
    "        stream = self.generator(file_list)\n",
    "        samples = self.reservoir_sampling(stream)\n",
    "        self.output_samples(samples)\n",
    "\n",
    "\n",
    "class AnnotDataProcessor:\n",
    "    def __init__(self, data1_file, data_folder, output_file):\n",
    "        self.data1_file = data1_file\n",
    "        self.data_folder = data_folder\n",
    "        self.output_file = output_file\n",
    "\n",
    "    def read_data1(self):\n",
    "        data1 = []\n",
    "        with open(self.data1_file) as f:\n",
    "            header = f.readline().strip() + \"\\tAF\\tSB\\n\"\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                data1.append(line)\n",
    "        return header, data1\n",
    "\n",
    "    def process_data(self):\n",
    "        header, data1 = self.read_data1()\n",
    "\n",
    "        for row in data1:\n",
    "            id = row[0]\n",
    "            filename = os.path.join(self.data_folder, f\"{id}_annot.txt\")\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename) as f:\n",
    "                    for line in f:\n",
    "                        line = line.strip().split()\n",
    "                        if line[0] == row[1]:\n",
    "                            row.append(line[3])\n",
    "                            row.append(line[4])\n",
    "                            break\n",
    "                    else:\n",
    "                        row.append(\"\")\n",
    "                        row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "        return header, data1\n",
    "\n",
    "    def write_output(self):\n",
    "        header, data1 = self.process_data()\n",
    "\n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            f.write(header)\n",
    "            for row in data1:\n",
    "                f.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder_path = \"/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/\"\n",
    "    checkid_file = \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Col_MAPLE_format_consensuses.txt\"\n",
    "    output_folder = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/Folder_MAPLE_allPos\"\n",
    "    sampler_output = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/COV_dataProcessing_allPos.txt\"\n",
    "    data_folder = \"/nfs/research/goldman/zihao/Datas/p1/File_5_annot/New_Decompress/\"\n",
    "    output_file = \"/nfs/research/goldman/zihao/Datas/p1_errorsProject_NEW/Folder_dataProcessing/ANNOT_dataProcessing_allPos.txt\"\n",
    "    nb_sample = 10\n",
    "    \n",
    "    # Run the CovFileProcessor\n",
    "    cov_processor = CovFileProcessor(folder_path, checkid_file, output_folder)\n",
    "    cov_processor.run()\n",
    "\n",
    "\n",
    "    def count_files(folder_path):\n",
    "        if not os.path.isdir(folder_path):\n",
    "            raise ValueError(\"Invalid folder path!\")\n",
    "\n",
    "        file_count = 0\n",
    "        for _, _, files in os.walk(folder_path):\n",
    "            file_count += len(files)\n",
    "\n",
    "        return file_count\n",
    "\n",
    "    folder_path = output_folder\n",
    "\n",
    "    try:\n",
    "        num_files = count_files(folder_path)\n",
    "        print('=========================== information =========================== ')\n",
    "        print(f\"The number of samples is: {num_files}\")\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "\n",
    "    \n",
    "    # Run the ReservoirSampler\n",
    "    sampler = ReservoirSampler(sampler_output, nb_sample)\n",
    "    sampler.process_files(output_folder)\n",
    "    \n",
    "    # Run the AnnotDataProcessor\n",
    "    processor = AnnotDataProcessor(sampler_output, data_folder, output_file)\n",
    "    processor.write_output()\n",
    "    \n",
    "    with open(output_file, 'r') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    print('=========================== information =========================== ')\n",
    "    print(f\"The total number of samples completed for sampling is: {line_count-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db90d5c",
   "metadata": {},
   "source": [
    "### ERR&ALL\n",
    "```bash\n",
    "sh /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/BASH_dataProcessing_errAndall.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Submit job 1\n",
    "echo \"Submitting job for OUTPUT\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forOutput.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forOutput.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forOutput.py'\n",
    "echo \"Job for output has been submitted\"\n",
    "\n",
    "# Submit job 2\n",
    "echo \"Submitting job for ANNOT\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forAnnot.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forAnnot.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forAnnot.py'\n",
    "echo \"Job for ANNOT has been submitted\"\n",
    "\n",
    "# Submit job 3\n",
    "echo \"Submitting job for COV\"\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_errPos_dataProcessing_forCov.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_errPos_dataProcessing_forCov.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errPos_dataProcessing/CODE_errPos_dataProcessing_forCov.py'\n",
    "echo \"Job for COV has been submitted\"\n",
    "\n",
    "# Submit job 4\n",
    "echo \"Submitting job for all Positions\"\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/errorChecking_allPos_dataProcessing.txt -o /nfs/research/goldman/zihao/N_errorsProject_1/Folder_errANDall_dataProcessing/Folder_Checking/outputChecking_allPos_dataProcessing.txt 'python3 /nfs/research/goldman/zihao/N_errorsProject_1/Folder_allPos_dataProcessing/CODE_allPos_dataProcessing.py'\n",
    "echo \"Job for all Positions has been submitted\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
