{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7abadf",
   "metadata": {},
   "source": [
    "### 1.Split processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a95af",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 20000 -e /nfs/research/goldman/zihao/compViridian_2/1_extract_sequence/pg_for_extract_2_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/compViridian_2/1_extract_sequence/pg_for_extract_2.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b8e769",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m---> 29\u001b[0m                 output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m             output_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile splitting and writing completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/nfs/research/goldman/zihao/Datas/p2_comp_viridian/1_extract_sequence'\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p2_comp_viridian/1_extract_sequence/split_files'\n",
    "files_per_txt = 10\n",
    "\n",
    "# Get the list of txt files in the folder\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Limit the number of output files to a maximum of 10\n",
    "num_output_files = min(len(txt_files), 10)\n",
    "\n",
    "# Calculate the number of files per output txt\n",
    "files_per_output = (len(txt_files) + num_output_files - 1) // num_output_files\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Split and write to txt files\n",
    "for i in range(num_output_files):\n",
    "    start_index = i * files_per_output\n",
    "    end_index = (i + 1) * files_per_output\n",
    "    output_file_path = os.path.join(output_folder, f'output_{i + 1}.fasta')\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        for file_name in txt_files[start_index:end_index]:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r') as input_file:\n",
    "                output_file.write(input_file.read())\n",
    "            output_file.write('\\n')\n",
    "\n",
    "print('File splitting and writing completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66cf6e",
   "metadata": {},
   "source": [
    "### 2.MAFFT alignment\n",
    "```bash\n",
    "sh /nfs/research/goldman/zihao/compViridian_2/2_alignment/Aligned.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9ae63",
   "metadata": {},
   "source": [
    "### 3.split_aligned_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e17a6d",
   "metadata": {},
   "source": [
    "```bash\n",
    "sh bash_for_split_aligned.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93737b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "file_path = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/new_version_MAY/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_new_all_estimatedErrors.txt\"\n",
    "data_set = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            data_set.add(line.strip())\n",
    "\n",
    "def process_file(input_filename, output_dir):\n",
    "    with open(input_filename, 'r') as file:\n",
    "        current_name = None\n",
    "        current_content = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_name is not None and current_name.split(\".\")[0] in data_set:\n",
    "                    output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "                    with open(output_path, \"w\") as out_file:\n",
    "                        out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "\n",
    "                current_name = line.strip()\n",
    "                current_content = []\n",
    "            else:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_name is not None and current_name.split(\".\")[0] in data_set:\n",
    "            output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "            with open(output_path, \"w\") as out_file:\n",
    "                out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process some files.\")\n",
    "    parser.add_argument(\"-i\", \"--input_file\", required=True, help=\"The input FASTA file to be processed\")\n",
    "    parser.add_argument(\"-o\", \"--output_dir\", required=True, help=\"The output directory to store processed files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_file(args.input_file, args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080b68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d367925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import gzip\n",
    "            \n",
    "def process_file(input_filename, output_dir):\n",
    "    with gzip.open(input_filename, 'rt') as file:\n",
    "        current_name = None\n",
    "        current_content = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if current_name is not None:\n",
    "                    output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "                    with open(output_path, \"w\") as out_file:\n",
    "                        out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "                \n",
    "                current_name = line.strip()\n",
    "                current_content = []\n",
    "            else:\n",
    "                current_content.append(line)\n",
    "\n",
    "        if current_name is not None:\n",
    "            output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "            with open(output_path, \"w\") as out_file:\n",
    "                out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process some files.\")\n",
    "    parser.add_argument(\"-i\", \"--input_file\", required=True, help=\"The input file to be processed\")\n",
    "    parser.add_argument(\"-o\", \"--output_dir\", required=True, help=\"The output directory to store processed files\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    process_file(args.input_file, args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32843e26",
   "metadata": {},
   "source": [
    "### 4.Combine into one DF\n",
    "- **Input**: \n",
    "```python\n",
    "viridian version: \"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Aligned_split/\"\n",
    "colman version: \"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned_split_May/\"\n",
    "```\n",
    "- **Output**: \n",
    "```python\n",
    "output path: /nfs/research/goldman/zihao/Datas/p2_comp_viridian/3_combination\n",
    "```\n",
    "- **Address**:\n",
    "```python\n",
    "/nfs/research/goldman/zihao/compViridian_2/3_combination/pg_combination.py\n",
    "```\n",
    "\n",
    "### Code block:\n",
    "###### Run the program\n",
    "```bash\n",
    "sh /nfs/research/goldman/zihao/compViridian_2/3_combination/bash_combination.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d8d42",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def read_sequence(file_path):\n",
    "    '''This function reads a FASTA file and returns a pandas DataFrame where\n",
    "    each row corresponds to the base and its position in the sequence'''\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sequence = ''.join(lines[1:]).replace('\\n', '')\n",
    "    sequence_list = [{'position': i+1, 'nucleotide': base} for i, base in enumerate(sequence)]\n",
    "\n",
    "    return pd.DataFrame(sequence_list)\n",
    "\n",
    "\n",
    "def process_batch(file_names, folder_path_1, folder_path_2, output_folder):\n",
    "    for file_name in file_names:\n",
    "        file_path_1 = os.path.join(folder_path_1, file_name)\n",
    "        df1 = read_sequence(file_path_1)\n",
    "        df1.rename(columns={'nucleotide': 'nucleotide_martin'}, inplace=True)\n",
    "\n",
    "        file_path_2 = os.path.join(folder_path_2, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path_2):\n",
    "            del df1\n",
    "            continue\n",
    "\n",
    "        df2 = read_sequence(file_path_2)\n",
    "        df2.rename(columns={'nucleotide': 'nucleotide_origin'}, inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(df1, df2, on='position')\n",
    "\n",
    "        merged_df['label_masked']=np.where((merged_df['nucleotide_martin'].isin(['-','n','m','r','w','s','y','k','v','h','d','b']))&(merged_df['nucleotide_origin'].isin(['-','n'])),1,0)\n",
    "        merged_df['label_mar']=np.where(merged_df['nucleotide_martin'].isin(['-','n','m','r','w','s','y','k','v','h','d','b']),1,0)\n",
    "        merged_df['label_ori']=np.where(merged_df['nucleotide_origin'].isin(['-','n']),1,0)\n",
    "        merged_df['label_same']=np.where((merged_df['nucleotide_martin']==merged_df['nucleotide_origin']),\n",
    "                                         np.where(merged_df['nucleotide_martin'].isin(['-','n','m','r','w','s','y','k','v','h','d','b']),2,1),\n",
    "                                         np.where((merged_df['label_masked']==1)|(merged_df['label_mar']==1)|(merged_df['label_ori']==1),2,0))\n",
    "\n",
    "        if os.path.exists(os.path.join(output_folder, file_name)):\n",
    "            del df1\n",
    "            del df2\n",
    "            del merged_df\n",
    "            continue\n",
    "\n",
    "        merged_df.to_csv(os.path.join(output_folder, file_name), sep='\\t', index=False)\n",
    "\n",
    "        del df1\n",
    "        del df2\n",
    "        del merged_df\n",
    "\n",
    "\n",
    "def main(output_folder, batch_num):\n",
    "    folder_path_1=\"/nfs/research/goldman/zihao/Datas/p2_comp_viridian/2_alignment/Aligned_split/\"\n",
    "    folder_path_2=\"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned_split_May/\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_names = os.listdir(folder_path_1)\n",
    "    batches = [file_names[i:i + 5000] for i in range(0, len(file_names), 5000)]\n",
    "\n",
    "    if batch_num < 1 or batch_num > len(batches):\n",
    "        raise ValueError('batch_num is out of range. It should be between 1 and {}'.format(len(batches)))\n",
    "\n",
    "    process_batch(batches[batch_num - 1], folder_path_1, folder_path_2, output_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Process a batch of FASTA files.')\n",
    "    parser.add_argument('output_folder', type=str, help='The output folder.')\n",
    "    parser.add_argument('batch_num', type=int, help='The batch number to process.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.output_folder, args.batch_num)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
