{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9846a69b",
   "metadata": {},
   "source": [
    "## 1_Extract_assembled_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import argparse\n",
    "\n",
    "base_path = \"/nfs/research/zi/mhunt/Viridian_wf_paper/Vdn_all_ena/Reads/\"\n",
    "\n",
    "output_files = {\n",
    "    'D': \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_D_new.txt\",\n",
    "    'S': \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_S_new.txt\",\n",
    "    'E': \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_E_new.txt\"\n",
    "}\n",
    "\n",
    "def process_folder(folder_path, output_file):\n",
    "    try:\n",
    "        subfolders = [subfolder for subfolder in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subfolder))]\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied when trying to access the directory: {folder_path}\")\n",
    "        return \n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if subfolder == \"vdn.v1.0.0\":\n",
    "            file_path = os.path.join(subfolder_path, \"consensus.fa.gz\")\n",
    "            try:\n",
    "                if os.path.exists(file_path):\n",
    "                    with gzip.open(file_path, 'rt') as f_in:\n",
    "                        content = f_in.read()\n",
    "                        with open(output_file, \"a\") as f_out:\n",
    "                            f_out.write(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with file {file_path}: {e}\")\n",
    "        else:\n",
    "            process_folder(subfolder_path, output_file)\n",
    "\n",
    "def main(folder):\n",
    "    output_file = output_files[folder]\n",
    "    with open(output_file, \"w\") as f_out:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        process_folder(folder_path, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process some files.\")\n",
    "    parser.add_argument('folder', choices=['D', 'S', 'E'], help=\"The folder to process\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7cdc5",
   "metadata": {},
   "source": [
    "## 1.2_Filter (Screen_of_MAPLE-treated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/new_version_MAY/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_new_all_estimatedErrors.txt\"\n",
    "data_set = set()\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith(\">\"):\n",
    "            data_set.add(line.strip())\n",
    "            \n",
    "def process_file(input_filename, output_dir):\n",
    "    with open(input_filename, \"r\") as file:\n",
    "        current_name = None\n",
    "        current_content = []\n",
    "        for line in file:\n",
    "            # This line is a sequence name\n",
    "            if line.startswith(\">\"):\n",
    "                 # If there was a sequence in data_set before, then save it to a file now\n",
    "                if current_name is not None and current_name.split(\".\")[0] in data_set:\n",
    "                    output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "                    with open(output_path, \"w\") as out_file:\n",
    "                        out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "                \n",
    "                # Update the name of the current sequence and clear the content list for next time\n",
    "                current_name = line.strip()\n",
    "                current_content = []\n",
    "            else:\n",
    "                # This line is part of the sequence and is added to the current content\n",
    "                current_content.append(line)\n",
    "\n",
    "        # Process end-of-file sequences\n",
    "        if current_name is not None and current_name.split(\".\")[0] in data_set:\n",
    "            output_path = os.path.join(output_dir, current_name.split(\".\")[0][1:] + \".txt\")\n",
    "            with open(output_path, \"w\") as out_file:\n",
    "                out_file.write(current_name + \"\\n\" + \"\".join(current_content))\n",
    "\n",
    "input_file = \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_D.txt\"\n",
    "output_folder = \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_D\"\n",
    "process_file(input_file, output_folder)\n",
    "\n",
    "## combine all files\n",
    "output_file = '/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_D/all_files_combined.fasta'\n",
    "\n",
    "txt_files = [f for f in os.listdir(output_folder) if f.endswith('.txt')]\n",
    "\n",
    "with open(output_file, 'w') as output:\n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(output_folder, txt_file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            output.write(content)\n",
    "            output.write('\\n')  # 在每个文件的内容后面添加换行符，以便区分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac6dde",
   "metadata": {},
   "source": [
    "## 1.2_Sequence_alignment\n",
    "\n",
    "## 1. split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49d73d",
   "metadata": {},
   "source": [
    "#### 1. Origin\n",
    "```bash\n",
    "sh bash_MAPLE_part.sh\n",
    "```\n",
    "```python\n",
    "save path: /nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned_split_May\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Process some files.')\n",
    "    parser.add_argument('--input_file', '-i', required=True, help='Path to input file')\n",
    "    parser.add_argument('--output_folder', '-o', required=True, help='Path to output folder')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    file_path = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/new_version_MAY/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_new_all_estimatedErrors.txt\"\n",
    "    input_file = args.input_file\n",
    "    output_folder = args.output_folder\n",
    "\n",
    "    data_set = set()\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                data_set.add(line.strip())\n",
    "\n",
    "    current_sequence = ''\n",
    "    current_name = None\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_sequence and current_name in data_set:\n",
    "                    output_file_path = os.path.join(output_folder, current_name[1:] + '.txt')\n",
    "                    with open(output_file_path, 'w') as output_file:\n",
    "                        output_file.write(current_name + '\\n')\n",
    "                        output_file.write(current_sequence)\n",
    "                current_name = line\n",
    "                current_sequence = ''\n",
    "            else:\n",
    "                current_sequence += line\n",
    "\n",
    "    # 处理最后一个序列\n",
    "    if current_sequence and current_name in data_set:\n",
    "        output_file_path = os.path.join(output_folder, current_name[1:] + '.txt')\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(current_name + '\\n')\n",
    "            output_file.write(current_sequence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a05c78",
   "metadata": {},
   "source": [
    "#### 2. Martin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b591cc",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Part1_2_for_assemble/Part2_Compare/2_Martin_part_E_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Part1_2_for_assemble/Part2_Compare/2.Martin_for_E.py'\n",
    "```\n",
    "\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Part1_2_for_assemble/Part2_Compare/2_Martin_part_S_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Part1_2_for_assemble/Part2_Compare/2.Martin_for_S.py'\n",
    "```\n",
    "\n",
    "```python\n",
    "save path: /nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/Aligned_split_May\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "def process_sequences(input_file, output_folder):\n",
    "    for record in SeqIO.parse(input_file, \"fasta\"):\n",
    "        sequence_name = record.id\n",
    "        sequence_data = str(record.seq)\n",
    "        sequence_name = sequence_name.replace(\".masked\", \"\")  # 移除\".masked\"后缀\n",
    "        save_sequence(sequence_name, sequence_data, output_folder)\n",
    "\n",
    "\n",
    "def save_sequence(sequence_name, sequence_data, output_folder):\n",
    "    output_file = os.path.join(output_folder, f\"{sequence_name}.txt\")\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(f\">{sequence_name}\\n\")\n",
    "        f.write(sequence_data)\n",
    "\n",
    "\n",
    "input_file = '/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/output_S_aligned.fasta'\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/Aligned_split_May'\n",
    "process_sequences(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0bf4f",
   "metadata": {},
   "source": [
    "## 2.MAFFT alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ac7d2",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub sh /nfs/research/goldman/zihao/errorsProject_1/Part1_2_for_assemble/Aligned.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a19b3bb",
   "metadata": {},
   "source": [
    "## 2.2_Combine into one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_sequence(file_path):\n",
    "    '''This function reads a FASTA file and returns a pandas DataFrame where\n",
    "    each row corresponds to the base and its position in the sequence'''\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sequence = ''.join(lines[1:]).replace('\\n', '')\n",
    "    sequence_list = [{'position': i, 'nucleotide': base} for i, base in enumerate(sequence)]\n",
    "\n",
    "    return pd.DataFrame(sequence_list)\n",
    "\n",
    "\n",
    "def main(output_folder):\n",
    "    folder_path_1 = \"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/Aligned_split_May/\"\n",
    "    folder_path_2 = \"/nfs/research/goldman/zihao/Datas/p1/File_5_consensus/Decompress/Aligned_split_May/\"\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    file_names = os.listdir(folder_path_1)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_path_1 = os.path.join(folder_path_1, file_name)\n",
    "        df1 = read_sequence(file_path_1)\n",
    "        df1.rename(columns={'nucleotide': 'nucleotide_martin'}, inplace=True)\n",
    "\n",
    "        file_path_2 = os.path.join(folder_path_2, file_name)\n",
    "\n",
    "        # Check if the file exists in folder_path_2, if not, skip to the next file\n",
    "        if not os.path.exists(file_path_2):\n",
    "            del df1\n",
    "            continue\n",
    "\n",
    "        df2 = read_sequence(file_path_2)\n",
    "        df2.rename(columns={'nucleotide': 'nucleotide_origin'}, inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(df1, df2, on='position')\n",
    "\n",
    "        # Adding decision columns\n",
    "        merged_df['label_same'] = np.where(merged_df['nucleotide_martin'] == merged_df['nucleotide_origin'], 1, 0)\n",
    "        merged_df['label_marked'] = np.where(\n",
    "            (merged_df['nucleotide_martin'].isin(['-', 'n'])) & \n",
    "            (merged_df['nucleotide_origin'].isin(['-', 'n'])), 1, 0)\n",
    "        merged_df['label_mar'] = np.where(merged_df['nucleotide_martin'].isin(['-', 'n']), 1, 0)\n",
    "        merged_df['label_ori'] = np.where(merged_df['nucleotide_origin'].isin(['-', 'n']), 1, 0)\n",
    "        \n",
    "        # Check if the file already exists in the output folder, if yes, skip to the next file\n",
    "        if os.path.exists(os.path.join(output_folder, file_name)):\n",
    "            del df1\n",
    "            del df2\n",
    "            del merged_df\n",
    "            continue\n",
    "\n",
    "        # Save the file to the specified output folder\n",
    "        merged_df.to_csv(os.path.join(output_folder, file_name), sep='\\t', index=False)\n",
    "\n",
    "        # To manage memory, clear variables that are no longer needed\n",
    "        del df1\n",
    "        del df2\n",
    "        del merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"/nfs/research/goldman/zihao/Datas/p1/Part1_2_for_assemble_data/2_combination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2609c",
   "metadata": {},
   "source": [
    "## 2.3_calculate percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de857a30",
   "metadata": {},
   "source": [
    "### Part1_for_all_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63a6ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both versions have the same nucleotide type: 95.106 %\n",
      "Both versions are marked: 0.279 %\n",
      "Only the martin version is marked: 0.903 %\n",
      "Only the original version is marked: 4.824 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = \"TEST/\"\n",
    "batch_size = 1000  # 每批次处理的文件数\n",
    "\n",
    "# 获取文件夹中的所有文件名\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# 初始化计数变量\n",
    "count_label_1_total = 0\n",
    "count_label_2_total = 0\n",
    "count_label_3_total = 0\n",
    "count_label_4_total = 0\n",
    "\n",
    "# 按批次处理文件\n",
    "num_batches = (len(file_names) + batch_size - 1) // batch_size  # 计算批次数量\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "    batch_files = file_names[start_idx:end_idx]\n",
    "\n",
    "    # 遍历批次中的每个文件\n",
    "    for file_name in batch_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        \n",
    "        # 计算每个文件中的label计数\n",
    "        count_label_1 = df['label'].value_counts().get(1, 0)\n",
    "        count_label_2 = df['label2'].value_counts().get(1, 0)\n",
    "        count_label_3 = df['label_mar'].value_counts().get(1, 0)\n",
    "        count_label_4 = df['label_ori'].value_counts().get(1, 0)\n",
    "        \n",
    "        # 累加计数到总计数变量\n",
    "        count_label_1_total += count_label_1\n",
    "        count_label_2_total += count_label_2\n",
    "        count_label_3_total += count_label_3\n",
    "        count_label_4_total += count_label_4\n",
    "\n",
    "# 打印总计数\n",
    "total_files = len(file_names)\n",
    "total_records = 29903 * total_files\n",
    "percentage_1 = round(count_label_1_total / total_records * 100, 3)\n",
    "percentage_2 = round(count_label_2_total / total_records * 100, 3)\n",
    "percentage_3 = round(count_label_3_total / total_records * 100, 3)\n",
    "percentage_4 = round(count_label_4_total / total_records * 100, 3)\n",
    "\n",
    "print(\"Both versions have the same nucleotide type:\", percentage_1, '%')\n",
    "print(\"Both versions are marked:\", percentage_2, '%')\n",
    "print(\"Only the martin version is marked:\", percentage_3, '%')\n",
    "print(\"Only the original version is marked:\", percentage_4, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155bfca",
   "metadata": {},
   "source": [
    "### Part2_for_err_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fe94ff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to output_data.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file_path, delimiter='\\t'):\n",
    "    \"\"\"Reads a file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path, delimiter=delimiter)\n",
    "\n",
    "def fetch_file_path(folder_path, file_id, extension='.txt'):\n",
    "    \"\"\"Builds the file path from the folder path and file id.\"\"\"\n",
    "    return os.path.join(folder_path, f'{file_id}{extension}')\n",
    "\n",
    "def match_row(df, column, value):\n",
    "    \"\"\"Finds rows in a DataFrame that match a specific value in a specific column.\"\"\"\n",
    "    return df[df[column] == value]\n",
    "\n",
    "def append_columns(df_source, df_target, index, columns):\n",
    "    \"\"\"Appends columns from a source DataFrame to a target DataFrame.\"\"\"\n",
    "    df_target.loc[index, columns] = df_source[columns].values.tolist()[0]\n",
    "\n",
    "def save_file(df, file_path, columns, delimiter='\\t', index=False):\n",
    "    \"\"\"Saves a DataFrame to a file.\"\"\"\n",
    "    df[columns].to_csv(file_path, sep=delimiter, index=index)\n",
    "\n",
    "# Specify paths and column names \n",
    "a_file_path = '/nfs/research/goldman/zihao/errorsProject_1/MAPLE/new_version_MAY/output_modified.txt'\n",
    "b_folder_path = \"TEST/\"\n",
    "b_column_names = ['position', 'nucleotide_martin', 'nucleotide_origin', 'label', 'label2', 'label_mar', 'label_ori']\n",
    "output_file_path = 'output_data.txt'\n",
    "\n",
    "# Load the data from the 'A' file\n",
    "a_data = read_file(a_file_path)\n",
    "\n",
    "# Iterate through each row of the 'A' data\n",
    "for index, row in a_data.iterrows():\n",
    "    # Fetch the path of the corresponding 'B' file\n",
    "    b_file_path = fetch_file_path(b_folder_path, row['ID'])\n",
    "    \n",
    "    # Continue to the next iteration if the 'B' file doesn't exist\n",
    "    if not os.path.isfile(b_file_path):\n",
    "        continue\n",
    "\n",
    "    # Load the data from the 'B' file\n",
    "    b_data = read_file(b_file_path)\n",
    "\n",
    "    # Find the matching row in the 'B' data\n",
    "    matched_row = match_row(b_data, 'position', row['Position'])\n",
    "\n",
    "    # Continue to the next iteration if no matching row was found\n",
    "    if matched_row.empty:\n",
    "        print(f'No matching row found in {b_file_path} for ID {row[\"ID\"]} and position {row[\"Position\"]}.')\n",
    "        continue\n",
    "\n",
    "    # Append the necessary columns to the 'A' data\n",
    "    append_columns(matched_row, a_data, index, b_column_names)\n",
    "\n",
    "# Save the modified 'A' data\n",
    "save_file(a_data, output_file_path, ['ID', 'Position'] + b_column_names)\n",
    "\n",
    "print(f\"Data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17dda8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both versions have the same nucleotide type: 100.0 %\n",
      "Both versions are marked: 0.0 %\n",
      "Only the martin version is marked: 0.0 %\n",
      "Only the original version is marked: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "### calculate_percentage\n",
    "test = pd.read_csv(output_file_path, sep='\\t')\n",
    "test = test.dropna()\n",
    "\n",
    "# 初始化计数变量\n",
    "count_label_1_total = sum(test['label'] == 1.0)\n",
    "count_label_2_total = sum(test['label2'] == 1.0)\n",
    "count_label_3_total = sum(test['label_mar'] == 1.0)\n",
    "count_label_4_total = sum(test['label_ori'] == 1.0)\n",
    "\n",
    "# 打印总计数\n",
    "total_records = len(test)\n",
    "percentage_1 = round(count_label_1_total / total_records * 100, 3)\n",
    "percentage_2 = round(count_label_2_total / total_records * 100, 3)\n",
    "percentage_3 = round(count_label_3_total / total_records * 100, 3)\n",
    "percentage_4 = round(count_label_4_total / total_records * 100, 3)\n",
    "\n",
    "print(\"Both versions have the same nucleotide type:\", percentage_1, '%')\n",
    "print(\"Both versions are marked:\", percentage_2, '%')\n",
    "print(\"Only the martin version is marked:\", percentage_3, '%')\n",
    "print(\"Only the original version is marked:\", percentage_4, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
