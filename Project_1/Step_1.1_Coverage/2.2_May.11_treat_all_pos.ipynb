{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5c4afd",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Coverage/Treat_all_pos_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_Treat_all_pos.py'\n",
    "```\n",
    "\n",
    "```bash\n",
    "sed -i 's|checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/output_modified.txt\"|checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"|g' Coverage_Treat_all_pos.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5916303",
   "metadata": {},
   "source": [
    "### Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20865a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, output_folder):\n",
    "    \"\"\"\n",
    "    Check the files in the given folder whose filenames contain the IDs in the specified files to the output folder.\n",
    "    \"\"\"\n",
    "    id_set = set()\n",
    "\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            shutil.copy(os.path.join(folder_path, filename), os.path.join(output_folder, filename))\n",
    "            \n",
    "def process_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Integrate and merge all data (in preparation for later sampling)\n",
    "    \"\"\"\n",
    "    # Get the list of file names in the input folder\n",
    "    file_names = os.listdir(input_folder)\n",
    "\n",
    "    # Create a new txt file to store the file content\n",
    "    with open(os.path.join(output_folder, \"Cov_RATIO.txt\"), \"wt\") as output_file:\n",
    "        # Create a csv writer object and set the delimiter as '\\t'\n",
    "        writer = csv.writer(output_file, delimiter='\\t')\n",
    "        # Write the column names to the output file\n",
    "        writer.writerow([\"ID\", \"Position\", \"Ratio\"])\n",
    "\n",
    "        # Loop through the first N files with the extension '.txt' in the input folder\n",
    "        for i, file_name in enumerate(file_names):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                # Extract the file ID from the file name\n",
    "                file_id = file_name.split(\"_\")[0]\n",
    "                # Open the file, skip the header, and read the Position and Ratio columns\n",
    "                with open(os.path.join(input_folder, file_name), \"r\") as f:\n",
    "                    file_lines = f.readlines()[1:]\n",
    "                    # Loop through each line and write the ID, Position, and Ratio to the output file\n",
    "                    for line in file_lines:\n",
    "                        columns = line.strip().split(\"\\t\")\n",
    "                        position = columns[0]\n",
    "                        ratio = columns[2]\n",
    "                        writer.writerow([file_id, position, ratio])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c648a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Decompress/'\n",
    "checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"\n",
    "middle_output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/PLOT_FOR_Coverage/'\n",
    "\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "check_files_with_id(folder_path, checkid_file, middle_output_folder)\n",
    "process_files(middle_output_folder, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214230",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134188",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_sampling_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Coverage/Coverage_sampling_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca156ba",
   "metadata": {},
   "source": [
    "想在每个位置从所有可能的ID中随机选择五个，而且数据量很大，不能一次性加载到内存中。在这种情况下，可以使用\"Reservoir Sampling\"算法，它是一个在有限内存下对大数据流进行随机采样的算法。\n",
    "\n",
    "下面是使用\"Reservoir Sampling\"的代码。注意，由于\"Reservoir Sampling\"的特性，这个方法会保证每个位置的每个ID有相等的被选中概率，但是并不能保证每个位置一定会有5个不同的ID，特别是当某个位置的ID数量小于5的时候。\n",
    "\n",
    "这种算法的基本思想是：对于每个新来的元素，我们以一定的概率决定是否将它包含在样本中。如果决定包含，我们就从当前的样本中随机选择一个元素并将其替换为新元素。随着数据流的进行，每个元素被选中的概率都是均等的。\n",
    "\n",
    "## Reservoir Sampling Algorithm\n",
    "\n",
    "Reservoir Sampling is a randomized algorithm used to select a random sample of `k` items from a stream or a large dataset of unknown size. The algorithm ensures that each item in the stream has an equal probability of being selected for the sample, regardless of the size of the dataset. Reservoir Sampling is particularly useful when the dataset is too large to fit into memory or when the size is unknown in advance.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "1. Initialize an empty reservoir of size `k` to store the sampled items.\n",
    "2. Read the stream or dataset one item at a time.\n",
    "3. For the first `k` items, simply add them to the reservoir.\n",
    "4. For the `i`th item (where `i > k`), generate a random number `j` between `1` and `i` (inclusive).\n",
    "   - If `j <= k`, replace the `j`th item in the reservoir with the `i`th item.\n",
    "   - Otherwise, ignore the `i`th item and continue to the next item.\n",
    "5. Repeat steps 4-5 until all items in the stream or dataset have been processed.\n",
    "6. The final reservoir contains a random sample of `k` items from the stream or dataset.\n",
    "\n",
    "The Reservoir Sampling algorithm ensures that each item in the stream has an equal probability of being selected for the sample. The probability of any specific item being in the final reservoir is `k` divided by the total number of items processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_with_data(data_file, output_file, sample_size):\n",
    "    \"\"\"\n",
    "    Sampling using Reservoir Sampling Algorithm (five samples of data are retained at each location)\n",
    "    \"\"\"\n",
    "    selected_data = collections.defaultdict(list)\n",
    "    ## <defaultdict object> to ensure that \n",
    "    ## the memory occupied by the old value will be reclaimed by the garbage collection mechanism.\n",
    "    \n",
    "    with open(data_file, 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "\n",
    "            id_, position, ratio = line.strip().split('\\t')\n",
    "            position = int(position)\n",
    "            ratio = float(ratio)\n",
    "\n",
    "            if len(selected_data[position]) < sample_size:\n",
    "                selected_data[position].append((id_, ratio))\n",
    "            else:\n",
    "                # Replace existing elements with a certain probability\n",
    "                s = int(random.uniform(0, len(selected_data[position])))\n",
    "                if s < sample_size:\n",
    "                    selected_data[position][s] = (id_, ratio)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(\"ID\\tPosition\\tRatio\\n\")\n",
    "        for position in selected_data:\n",
    "            for id_, ratio in selected_data[position]:\n",
    "                file.write(f\"{id_}\\t{position}\\t{ratio}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "data_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/Cov_RATIO.txt'\n",
    "output_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/selected_data.txt'\n",
    "sample_size = 5  # This is the number of samples you want to retain at each location\n",
    "experiment_with_data(data_file, output_file, sample_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
