{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a49158",
   "metadata": {},
   "source": [
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/Treat_all_pos_errorChecking_error.txt \n",
    "'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_Treat_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02f0aa",
   "metadata": {},
   "source": [
    "## Final Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "\n",
    "def check_files_with_id(folder_path, checkid_file, output_folder):\n",
    "    \"\"\"\n",
    "    Check the files in the given folder whose filenames contain the IDs in the specified files to the output folder.\n",
    "    \"\"\"\n",
    "    id_set = set()\n",
    "\n",
    "    with open(checkid_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                id_set.add(line[1:])\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(id_str in filename for id_str in id_set):\n",
    "            shutil.copy(os.path.join(folder_path, filename), os.path.join(output_folder, filename))\n",
    "            \n",
    "\n",
    "def process_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Integrate and merge all data (in preparation for later sampling)\n",
    "    \"\"\"\n",
    "    file_names = os.listdir(input_folder)\n",
    "\n",
    "    # Create a new txt file to store the file content\n",
    "    with open(os.path.join(output_folder, \"Annot_RATIO.txt\"), \"wt\") as output_file:\n",
    "        # Create a csv writer object and set the delimiter as '\\t'\n",
    "        writer = csv.writer(output_file, delimiter='\\t')\n",
    "        # Write the column names to the output file\n",
    "        writer.writerow([\"ID\", \"Position\", \"AF_Ratio\", \"SB_Ratio\"])\n",
    "\n",
    "        # Loop through the first N files with the extension '.txt' in the input folder\n",
    "        for i, file_name in enumerate(file_names):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                # Extract the file ID from the file name\n",
    "                file_id = file_name.split(\"_\")[0]\n",
    "                # Open the file, skip the header, and read the Position and Ratio columns\n",
    "                with open(os.path.join(input_folder, file_name), \"r\") as f:\n",
    "                    file_lines = f.readlines()[1:]\n",
    "                    # Loop through each line and write the ID, Position, and Ratio to the output file\n",
    "                    for line in file_lines:\n",
    "                        columns = line.strip().split(\"\\t\")\n",
    "                        position = columns[0]\n",
    "                        af = columns[3]\n",
    "                        sb = columns[4]\n",
    "                        writer.writerow([file_id, position, af, sb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0332af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Decompress/'\n",
    "checkid_file = \"/nfs/research/goldman/zihao/errorsProject_1/MAPLE/TEST_50000/MAPLE0.3.2_rateVar_errors_realData_checkingErrors_50000_estimatedErrors.txt\"\n",
    "middle_output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/PLOT_FOR_Annot/'\n",
    "\n",
    "output_folder = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run the function\n",
    "check_files_with_id(folder_path, checkid_file, middle_output_folder)\n",
    "process_files(middle_output_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29faefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "911357ed",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "```bash\n",
    "bsub -M 2000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/RS_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/RS_For_Annot.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c1094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "def experiment_with_data(data_file, output_file):\n",
    "    selected_data = collections.defaultdict(list) \n",
    "    ## <defaultdict object> to ensure that \n",
    "    ## the memory occupied by the old value will be reclaimed by the garbage collection mechanism.\n",
    "\n",
    "    with open(data_file, 'r') as file:\n",
    "        next(file)\n",
    "        \n",
    "        # ！！！！！！！！！！！\n",
    "        line_count = 0  # 记录读取的行数\n",
    "        # ！！！！！！！！！！！\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            # ！！！！！！！！！！！\n",
    "            line_count += 1\n",
    "            if line_count > 299030:\n",
    "                break  # 停止读取数据\n",
    "            # ！！！！！！！！！！！\n",
    "\n",
    "            id_, position, af_ratio, sb_ratio = line.strip().split('\\t')\n",
    "            position = int(position)\n",
    "            af_ratio = float(af_ratio)\n",
    "            sb_ratio = float(sb_ratio)\n",
    "\n",
    "            if len(selected_data[position]) < 5:\n",
    "                selected_data[position].append((id_, af_ratio, sb_ratio))\n",
    "            else:\n",
    "                # Replace existing elements with a certain probability\n",
    "                s = int(random.uniform(0, len(selected_data[position])))\n",
    "                if s < 5:\n",
    "                    selected_data[position][s] = (id_, af_ratio, sb_ratio)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(\"ID\\tPosition\\tAF_Ratio\\tSB_Ratio\\n\")\n",
    "        for position in selected_data:\n",
    "            for id_, af_ratio, sb_ratio in selected_data[position]:\n",
    "                file.write(f\"{id_}\\t{position}\\t{af_ratio}\\t{sb_ratio}\\n\")\n",
    "\n",
    "    print(\"选择的数据已保存到文件：\", output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf0420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择的数据已保存到文件： selected_data_1.txt\n"
     ]
    }
   ],
   "source": [
    "# 使用方法\n",
    "data_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Annot_RATIO.txt'\n",
    "output_file = 'selected_data_1.txt'\n",
    "experiment_with_data(data_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df35fa",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "```bash\n",
    "bsub -M 200000 -e /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_sampling_all_errorChecking_error.txt 'python3 /nfs/research/goldman/zihao/errorsProject_1/Annot/Annot_sampling_all_pos.py'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7c78ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_files(data_file, input_file, output_file, chunk_size=10000, max_rows=299030):\n",
    "    \"\"\"\n",
    "    This function reads two input files and merges specific columns from the second file \n",
    "    into the first file based on matching ID and Position. The merged data is then saved to an output file.\n",
    "    \"\"\"\n",
    "    # Initialize the dictionary to store AF and SB values\n",
    "    af_sb_dict = {}\n",
    "\n",
    "    # Read the second input file and store matching data\n",
    "    with open(input_file, 'r') as file:\n",
    "        next(file)  # Skip the title line\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= max_rows:  # Limit the number of rows read\n",
    "                break\n",
    "            fields = line.strip().split('\\t')\n",
    "            id_position_str = '\\t'.join(fields[:2])\n",
    "            af_sb_dict[id_position_str] = fields[2:4]\n",
    "    \n",
    "    # Create a function to process a chunk of data\n",
    "    def process_chunk(chunk):\n",
    "        chunk['AF'] = np.nan\n",
    "        chunk['SB'] = np.nan\n",
    "        for index, row in chunk.iterrows():\n",
    "            id_position_str = '\\t'.join(map(str, [row['ID'], row['Position']]))\n",
    "            if id_position_str in af_sb_dict:\n",
    "                chunk.at[index, 'AF'], chunk.at[index, 'SB'] = af_sb_dict[id_position_str]\n",
    "            else:\n",
    "                chunk.at[index, 'AF'], chunk.at[index, 'SB'] = 'None', 'None'  # Fill in None for unmatched data\n",
    "        return chunk\n",
    "\n",
    "    # Process the first input file in chunks and write each processed chunk to the output file\n",
    "    first_chunk = True\n",
    "    for chunk in pd.read_csv(data_file, sep='\\t', chunksize=chunk_size):\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        if first_chunk:\n",
    "            processed_chunk.to_csv(output_file, sep='\\t', index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            processed_chunk.to_csv(output_file, sep='\\t', index=False, mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c007b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths of your files\n",
    "data_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_coverage/selected_data.txt'\n",
    "input_file = '/nfs/research/goldman/zihao/Datas/p1/File_5_annot/Annot_RATIO.txt'\n",
    "output_file = 'test.txt'  # Replace with the actual path of the output file\n",
    "\n",
    "# Call the function\n",
    "merge_files(data_file, input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c073e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108f4237",
   "metadata": {},
   "source": [
    "\n",
    "这段代码是一个用于处理文件的循环。它使用了Pandas库来逐块读取一个输入文件（data_file），并将每个处理过的块写入一个输出文件（output_file）。\n",
    "\n",
    "代码中的循环使用了pd.read_csv()函数来逐块读取输入文件。参数sep='\\t'指定了文件的分隔符为制表符。chunksize参数确定了每个块的大小。\n",
    "\n",
    "在循环中，每个块被传递给process_chunk()函数进行处理，并返回一个经过处理的块（processed_chunk）。\n",
    "\n",
    "代码中的逻辑判断用于确定是将处理过的块写入输出文件作为新文件（first_chunk = True），还是将其追加到已存在的文件中（first_chunk = False）。\n",
    "\n",
    "如果是第一个块（first_chunk = True），则调用processed_chunk.to_csv()将处理过的块写入输出文件。index=False参数指定不将行索引写入文件。\n",
    "\n",
    "如果不是第一个块（first_chunk = False），则调用processed_chunk.to_csv()将处理过的块追加到输出文件中。mode='a'参数指定以追加模式打开文件，header=False参数指定不将列名写入文件。\n",
    "\n",
    "这段代码的目的是逐块处理输入文件的内容并将处理结果写入输出文件，以便有效地处理大型数据集而不会消耗太多内存。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
