{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701fe60f",
   "metadata": {},
   "source": [
    "## <center>Data's Preparation for 5 files</center>\n",
    "\n",
    "\n",
    "| **Label** | **start time** | **finish time** | **last modified** |\n",
    "|:--------------:|:-----------:|:-----------:|:----------------:|\n",
    "|   Project 1   |  2023-04-11 |  2023-04-11 |   2023-04-11     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa26bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Apr.11_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926604cb-937b-4d2c-bffd-47ebcb7ef973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0069d0",
   "metadata": {},
   "source": [
    "# Data Processing.1_Picking_5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8c8dbf-0f10-4c4f-aebf-032a2620a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>number of files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>DRR428582</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>ERR10008306</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>ERR10008309</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>ERR10008311</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>ERR10008328</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          run_id  number of files\n",
       "126    DRR428582                5\n",
       "127  ERR10008306                5\n",
       "128  ERR10008309                5\n",
       "129  ERR10008311                5\n",
       "130  ERR10008328                5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/nfs/research/goldman/zihao/Datas/p1/Origin_Data/analysis_run_list.txt', delimiter='\\t')\n",
    "df = df[df['number of files'] == 5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = set(df['run_id'])\n",
    "\n",
    "# Create output file and write to table header\n",
    "with open('/nfs/research/goldman/zihao/Datas/p1/Origin_Data/file_5_script.txt', 'w') as f:\n",
    "    f.write(\"run_id\\tdata_file\\tFTP_path\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63018c12-d8e6-484e-ade8-ce92ea0694bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Time taken for chunk 1: 17.14 seconds\n",
      "Processing chunk 2\n",
      "Time taken for chunk 2: 16.09 seconds\n",
      "Processing chunk 3\n",
      "Time taken for chunk 3: 15.67 seconds\n",
      "Processing chunk 4\n",
      "Time taken for chunk 4: 13.76 seconds\n",
      "Processing chunk 5\n",
      "Time taken for chunk 5: 13.62 seconds\n",
      "Processing chunk 6\n",
      "Time taken for chunk 6: 14.16 seconds\n",
      "Processing chunk 7\n",
      "Time taken for chunk 7: 13.77 seconds\n",
      "Processing chunk 8\n",
      "Time taken for chunk 8: 13.96 seconds\n",
      "Processing chunk 9\n",
      "Time taken for chunk 9: 16.32 seconds\n",
      "Processing chunk 10\n",
      "Time taken for chunk 10: 16.13 seconds\n",
      "Processing chunk 11\n",
      "Time taken for chunk 11: 14.01 seconds\n",
      "Processing chunk 12\n",
      "Time taken for chunk 12: 14.14 seconds\n",
      "Processing chunk 13\n",
      "Time taken for chunk 13: 14.19 seconds\n",
      "Processing chunk 14\n",
      "Time taken for chunk 14: 14.38 seconds\n",
      "Processing chunk 15\n",
      "Time taken for chunk 15: 14.24 seconds\n",
      "Processing chunk 16\n",
      "Time taken for chunk 16: 14.08 seconds\n",
      "Processing chunk 17\n",
      "Time taken for chunk 17: 13.95 seconds\n",
      "Processing chunk 18\n",
      "Time taken for chunk 18: 13.92 seconds\n",
      "Processing chunk 19\n",
      "Time taken for chunk 19: 13.94 seconds\n",
      "Processing chunk 20\n",
      "Time taken for chunk 20: 14.06 seconds\n",
      "Processing chunk 21\n",
      "Time taken for chunk 21: 13.99 seconds\n",
      "Processing chunk 22\n",
      "Time taken for chunk 22: 14.29 seconds\n",
      "Processing chunk 23\n",
      "Time taken for chunk 23: 13.92 seconds\n",
      "Processing chunk 24\n",
      "Time taken for chunk 24: 13.98 seconds\n",
      "Processing chunk 25\n",
      "Time taken for chunk 25: 14.28 seconds\n",
      "Processing chunk 26\n",
      "Time taken for chunk 26: 13.93 seconds\n",
      "Processing chunk 27\n",
      "Time taken for chunk 27: 14.06 seconds\n",
      "Processing chunk 28\n",
      "Time taken for chunk 28: 14.08 seconds\n",
      "Processing chunk 29\n",
      "Time taken for chunk 29: 14.01 seconds\n",
      "Processing chunk 30\n",
      "Time taken for chunk 30: 14.23 seconds\n",
      "Processing chunk 31\n",
      "Time taken for chunk 31: 14.26 seconds\n",
      "Processing chunk 32\n",
      "Time taken for chunk 32: 13.93 seconds\n",
      "Processing chunk 33\n",
      "Time taken for chunk 33: 14.10 seconds\n",
      "Processing chunk 34\n",
      "Time taken for chunk 34: 14.06 seconds\n",
      "Processing chunk 35\n",
      "Time taken for chunk 35: 13.81 seconds\n",
      "Processing chunk 36\n",
      "Time taken for chunk 36: 13.93 seconds\n",
      "Processing chunk 37\n",
      "Time taken for chunk 37: 14.08 seconds\n",
      "Processing chunk 38\n",
      "Time taken for chunk 38: 14.44 seconds\n",
      "Processing chunk 39\n",
      "Time taken for chunk 39: 13.95 seconds\n",
      "Processing chunk 40\n",
      "Time taken for chunk 40: 14.69 seconds\n",
      "Processing chunk 41\n",
      "Time taken for chunk 41: 14.33 seconds\n",
      "Processing chunk 42\n",
      "Time taken for chunk 42: 14.07 seconds\n",
      "Processing chunk 43\n",
      "Time taken for chunk 43: 13.84 seconds\n",
      "Processing chunk 44\n",
      "Time taken for chunk 44: 14.02 seconds\n",
      "Processing chunk 45\n",
      "Time taken for chunk 45: 14.02 seconds\n",
      "Processing chunk 46\n",
      "Time taken for chunk 46: 14.07 seconds\n",
      "Processing chunk 47\n",
      "Time taken for chunk 47: 14.15 seconds\n",
      "Processing chunk 48\n",
      "Time taken for chunk 48: 13.93 seconds\n",
      "Processing chunk 49\n",
      "Time taken for chunk 49: 14.28 seconds\n",
      "Processing chunk 50\n",
      "Time taken for chunk 50: 13.90 seconds\n",
      "Processing chunk 51\n",
      "Time taken for chunk 51: 13.91 seconds\n",
      "Processing chunk 52\n",
      "Time taken for chunk 52: 13.89 seconds\n",
      "Processing chunk 53\n",
      "Time taken for chunk 53: 17.19 seconds\n",
      "Processing chunk 54\n",
      "Time taken for chunk 54: 17.84 seconds\n",
      "Processing chunk 55\n",
      "Time taken for chunk 55: 22.69 seconds\n",
      "Processing chunk 56\n",
      "Time taken for chunk 56: 26.48 seconds\n",
      "Processing chunk 57\n",
      "Time taken for chunk 57: 26.08 seconds\n",
      "Processing chunk 58\n",
      "Time taken for chunk 58: 26.51 seconds\n",
      "Processing chunk 59\n",
      "Time taken for chunk 59: 26.30 seconds\n",
      "Processing chunk 60\n",
      "Time taken for chunk 60: 24.05 seconds\n",
      "Processing chunk 61\n",
      "Time taken for chunk 61: 14.12 seconds\n",
      "Processing chunk 62\n",
      "Time taken for chunk 62: 13.85 seconds\n",
      "Processing chunk 63\n",
      "Time taken for chunk 63: 14.29 seconds\n",
      "Processing chunk 64\n",
      "Time taken for chunk 64: 13.99 seconds\n",
      "Processing chunk 65\n",
      "Time taken for chunk 65: 18.54 seconds\n",
      "Processing chunk 66\n",
      "Time taken for chunk 66: 22.59 seconds\n",
      "Processing chunk 67\n",
      "Time taken for chunk 67: 25.33 seconds\n",
      "Processing chunk 68\n",
      "Time taken for chunk 68: 23.15 seconds\n",
      "Processing chunk 69\n",
      "Time taken for chunk 69: 14.39 seconds\n",
      "Processing chunk 70\n",
      "Time taken for chunk 70: 22.87 seconds\n",
      "Processing chunk 71\n",
      "Time taken for chunk 71: 26.59 seconds\n",
      "Processing chunk 72\n",
      "Time taken for chunk 72: 20.30 seconds\n",
      "Processing chunk 73\n",
      "Time taken for chunk 73: 21.49 seconds\n",
      "Processing chunk 74\n",
      "Time taken for chunk 74: 23.70 seconds\n",
      "Processing chunk 75\n",
      "Time taken for chunk 75: 21.88 seconds\n",
      "Processing chunk 76\n",
      "Time taken for chunk 76: 23.06 seconds\n",
      "Processing chunk 77\n",
      "Time taken for chunk 77: 23.51 seconds\n",
      "Processing chunk 78\n",
      "Time taken for chunk 78: 25.81 seconds\n",
      "Processing chunk 79\n",
      "Time taken for chunk 79: 26.11 seconds\n",
      "Processing chunk 80\n",
      "Time taken for chunk 80: 22.67 seconds\n",
      "Processing chunk 81\n",
      "Time taken for chunk 81: 26.94 seconds\n",
      "Processing chunk 82\n",
      "Time taken for chunk 82: 26.51 seconds\n",
      "Processing chunk 83\n",
      "Time taken for chunk 83: 26.96 seconds\n",
      "Processing chunk 84\n",
      "Time taken for chunk 84: 27.07 seconds\n",
      "Processing chunk 85\n",
      "Time taken for chunk 85: 27.12 seconds\n",
      "Processing chunk 86\n",
      "Time taken for chunk 86: 26.45 seconds\n",
      "Processing chunk 87\n",
      "Time taken for chunk 87: 26.48 seconds\n",
      "Processing chunk 88\n",
      "Time taken for chunk 88: 26.28 seconds\n",
      "Processing chunk 89\n",
      "Time taken for chunk 89: 26.55 seconds\n",
      "Processing chunk 90\n",
      "Time taken for chunk 90: 48.96 seconds\n",
      "Processing chunk 91\n",
      "Time taken for chunk 91: 40.91 seconds\n",
      "Processing chunk 92\n",
      "Time taken for chunk 92: 17.47 seconds\n"
     ]
    }
   ],
   "source": [
    "# Read the data file chunk by chunk\n",
    "for i, chunk in enumerate(pd.read_csv('/nfs/research/goldman/zihao/Datas/p1/Origin_Data/analysis_full_06.04.23.txt', delimiter='\\t', chunksize=100000)):\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "    # Iterate over each row of data\n",
    "    for index, row in chunk.iterrows():\n",
    "        # Check if the DataFrame contains the 'run_id' column\n",
    "        if 'run_id' in chunk.columns:\n",
    "            # Compare the \"run_id\" in the other file\n",
    "            if row['run_id'] in test: #df['run_id'].values: # list -> set\n",
    "                # If it exists, save run_id and FTP_path and write to a text file\n",
    "                data = {'run_id': row['run_id'], 'data_file': row['data_file'], 'FTP_path': row['FTP_path']}\n",
    "                with open('../Datas/file_5.txt', 'a') as f:\n",
    "                    f.write(f\"{data['run_id']}\\t{data['data_file']}\\t{data['FTP_path']}\\n\")\n",
    "        # Delete the row from memory\n",
    "        del row\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for chunk {i+1}: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294f78e",
   "metadata": {},
   "source": [
    "### De-duplicate based on data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete by the same file name, keeping only the first\n",
    "df.drop_duplicates(subset=[\"data_file\"], keep=\"first\", inplace=True)\n",
    "print(df['run_id'].value_counts().unique())\n",
    "df.to_csv(\"/nfs/research/goldman/zihao/Datas/p1/Origin_Data/file_5.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c7bcc",
   "metadata": {},
   "source": [
    "# Data Processing.2_Splitting_files_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a931eb40",
   "metadata": {},
   "source": [
    "### 1. consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[df[\"data_file\"].str.contains(\"consensus\")]\n",
    "print(df_1.shape)\n",
    "\n",
    "df_1.to_csv(\"/nfs/research/goldman/zihao/Datas/p1/File_5_Consensus.txt\", sep=\"\\t\", index=False)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ff6ad",
   "metadata": {},
   "source": [
    "### 2. coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c584e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df[df[\"data_file\"].str.contains(\"coverage\")]\n",
    "print(df_2.shape)\n",
    "\n",
    "df_2.to_csv(\"/nfs/research/goldman/zihao/Datas/p1/File_5_Coverage.txt\", sep=\"\\t\", index=False)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa27f7",
   "metadata": {},
   "source": [
    "### 3. annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e92e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df[df[\"data_file\"].str.contains(\"annot\")]\n",
    "print(df_3.shape)\n",
    "\n",
    "df_3.to_csv(\"/nfs/research/goldman/zihao/Datas/p1/File_5_Annot.txt\", sep=\"\\t\", index=False)\n",
    "df_3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
